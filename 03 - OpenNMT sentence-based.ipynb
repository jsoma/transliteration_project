{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "02 - OpenNMT - sentence-based.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j27NAQLilgd",
        "colab_type": "text"
      },
      "source": [
        "# Creating test and train datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlzmLwVPiv18",
        "colab_type": "code",
        "outputId": "b14a2fea-2d34-467d-f1eb-ce2d4e858ba4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!unzip Archive.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  Archive.zip\n",
            "  inflating: original.txt            \n",
            "  inflating: transliterated.txt      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8ZrK5seWGcJ0",
        "outputId": "1f1be18b-093a-4f8c-ee64-feafe787e4ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "original = open(\"original.txt\").read()\n",
        "transliterated = open(\"transliterated.txt\").read()\n",
        "\n",
        "words = pd.DataFrame({\n",
        "    'orig': re.split(r'\\n', original),\n",
        "    'trans': re.split(r'\\n', transliterated)\n",
        "})\n",
        "words.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>orig</th>\n",
              "      <th>trans</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ቀዝቃዛ ውኃ የዛለችን ነፍስ እንደሚያረካ ሁሉከሩቅ አገር የመጣ መልካም ወ...</td>\n",
              "      <td>kazeqaza weha yazalatchene nefsi inidamiyaraka...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>በመልካም ሁኔታ የሚያስተዳድሩ በተለይ ደግሞ በመናገርና በማስተማር ተግተው...</td>\n",
              "      <td>bemalkam huneta jamiyasitadaderu batelaye dagi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>መጽሐፍ ቅዱስ በመጨረሻዎቹ ቀናት ክፋት በከፍተኛ ሁኔታ እንደሚባባስ ይነግ...</td>\n",
              "      <td>mashefe qduse bemeceraxawotchu qana'te kefat b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>የአክሱም ሐውልት ሁለተኛው ክፋይ አክሱም ገባ</td>\n",
              "      <td>ye'akesum hawlte huletanaw kfaye akisume geba</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>የሴም ወንዶች ልጆች ኤላም፣ አሹር፣ አርፋክስድ፣ ሉድ እና አራም ነበሩ።</td>\n",
              "      <td>yasyeme wanidwoce lgoce 'elam, `ashure, `arifa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                orig                                              trans\n",
              "0  ቀዝቃዛ ውኃ የዛለችን ነፍስ እንደሚያረካ ሁሉከሩቅ አገር የመጣ መልካም ወ...  kazeqaza weha yazalatchene nefsi inidamiyaraka...\n",
              "1  በመልካም ሁኔታ የሚያስተዳድሩ በተለይ ደግሞ በመናገርና በማስተማር ተግተው...  bemalkam huneta jamiyasitadaderu batelaye dagi...\n",
              "2  መጽሐፍ ቅዱስ በመጨረሻዎቹ ቀናት ክፋት በከፍተኛ ሁኔታ እንደሚባባስ ይነግ...  mashefe qduse bemeceraxawotchu qana'te kefat b...\n",
              "3                       የአክሱም ሐውልት ሁለተኛው ክፋይ አክሱም ገባ      ye'akesum hawlte huletanaw kfaye akisume geba\n",
              "4      የሴም ወንዶች ልጆች ኤላም፣ አሹር፣ አርፋክስድ፣ ሉድ እና አራም ነበሩ።  yasyeme wanidwoce lgoce 'elam, `ashure, `arifa..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy6wJs6W4mv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_sentence(text):\n",
        "  return ' '.join('~'.join(text.strip().split(' ')))\n",
        "\n",
        "def decode_sentence(text):\n",
        "  return text.strip().replace(' ', '').replace('~', ' ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6sUbhu3ilgm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words.orig = words.orig.apply(encode_sentence)\n",
        "words.trans = words.trans.apply(encode_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqjZngkYilgq",
        "colab_type": "code",
        "outputId": "d8a9eaca-eec5-48f9-d7b2-aa48108d94e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "words.head(10)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>orig</th>\n",
              "      <th>trans</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ቀ ዝ ቃ ዛ ~ ው ኃ ~ የ ዛ ለ ች ን ~ ነ ፍ ስ ~ እ ን ደ ሚ ያ ...</td>\n",
              "      <td>k a z e q a z a ~ w e h a ~ y a z a l a t c h ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>በ መ ል ካ ም ~ ሁ ኔ ታ ~ የ ሚ ያ ስ ተ ዳ ድ ሩ ~ በ ተ ለ ይ ...</td>\n",
              "      <td>b e m a l k a m ~ h u n e t a ~ j a m i y a s ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>መ ጽ ሐ ፍ ~ ቅ ዱ ስ ~ በ መ ጨ ረ ሻ ዎ ቹ ~ ቀ ና ት ~ ክ ፋ ...</td>\n",
              "      <td>m a s h e f e ~ q d u s e ~ b e m e c e r a x ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>የ አ ክ ሱ ም ~ ሐ ው ል ት ~ ሁ ለ ተ ኛ ው ~ ክ ፋ ይ ~ አ ክ ...</td>\n",
              "      <td>y e ' a k e s u m ~ h a w l t e ~ h u l e t a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>የ ሴ ም ~ ወ ን ዶ ች ~ ል ጆ ች ~ ኤ ላ ም ፣ ~ አ ሹ ር ፣ ~ ...</td>\n",
              "      <td>y a s y e m e ~ w a n i d w o c e ~ l g o c e ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ለ ሚ መ ረ ም ሩ ኝ ~ የ ማ ቀ ር በ ው ~ የ መ ከ ላ ከ ያ ~ መ ...</td>\n",
              "      <td>l a m i m a r e m e r u n ~ y a m a q e r e b ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ያ ለ ነ ቀ ፋ ~ የ ሚ መ ላ ለ ስ ፣ ት ክ ክ ል ~ የ ሆ ነ ው ን ...</td>\n",
              "      <td>y a l a n a q e f a ~ y a m i m a l a l e s , ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ገ ና ~ ሳ ይ ጣ ሩ ~ እ መ ል ስ ላ ቸ ዋ ለ ሁ ፤ እ የ ተ ና ገ ...</td>\n",
              "      <td>g e n a ~ s a y i t a r u ~ i m a l i s e l a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>በ ዚ ህ ~ ጊ ዜ ~ ደ ቀ ~ መ ዛ ሙ ር ቱ ~ “ ጌ ታ ~ ሆ ይ ፣ ...</td>\n",
              "      <td>b a z i h ~ g i z e ~ d a q e ~ m e z a m u r ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ል ብ ህ ~ በ አ ም ላ ክ ~ ፊ ት ~ ቀ ና ~ ስ ላ ል ሆ ነ ~ በ ...</td>\n",
              "      <td>l i b i h e ~ b e a m i l a k i ~ f i t e ~ k ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                orig                                              trans\n",
              "0  ቀ ዝ ቃ ዛ ~ ው ኃ ~ የ ዛ ለ ች ን ~ ነ ፍ ስ ~ እ ን ደ ሚ ያ ...  k a z e q a z a ~ w e h a ~ y a z a l a t c h ...\n",
              "1  በ መ ል ካ ም ~ ሁ ኔ ታ ~ የ ሚ ያ ስ ተ ዳ ድ ሩ ~ በ ተ ለ ይ ...  b e m a l k a m ~ h u n e t a ~ j a m i y a s ...\n",
              "2  መ ጽ ሐ ፍ ~ ቅ ዱ ስ ~ በ መ ጨ ረ ሻ ዎ ቹ ~ ቀ ና ት ~ ክ ፋ ...  m a s h e f e ~ q d u s e ~ b e m e c e r a x ...\n",
              "3  የ አ ክ ሱ ም ~ ሐ ው ል ት ~ ሁ ለ ተ ኛ ው ~ ክ ፋ ይ ~ አ ክ ...  y e ' a k e s u m ~ h a w l t e ~ h u l e t a ...\n",
              "4  የ ሴ ም ~ ወ ን ዶ ች ~ ል ጆ ች ~ ኤ ላ ም ፣ ~ አ ሹ ር ፣ ~ ...  y a s y e m e ~ w a n i d w o c e ~ l g o c e ...\n",
              "5  ለ ሚ መ ረ ም ሩ ኝ ~ የ ማ ቀ ር በ ው ~ የ መ ከ ላ ከ ያ ~ መ ...  l a m i m a r e m e r u n ~ y a m a q e r e b ...\n",
              "6  ያ ለ ነ ቀ ፋ ~ የ ሚ መ ላ ለ ስ ፣ ት ክ ክ ል ~ የ ሆ ነ ው ን ...  y a l a n a q e f a ~ y a m i m a l a l e s , ...\n",
              "7  ገ ና ~ ሳ ይ ጣ ሩ ~ እ መ ል ስ ላ ቸ ዋ ለ ሁ ፤ እ የ ተ ና ገ ...  g e n a ~ s a y i t a r u ~ i m a l i s e l a ...\n",
              "8  በ ዚ ህ ~ ጊ ዜ ~ ደ ቀ ~ መ ዛ ሙ ር ቱ ~ “ ጌ ታ ~ ሆ ይ ፣ ...  b a z i h ~ g i z e ~ d a q e ~ m e z a m u r ...\n",
              "9  ል ብ ህ ~ በ አ ም ላ ክ ~ ፊ ት ~ ቀ ና ~ ስ ላ ል ሆ ነ ~ በ ...  l i b i h e ~ b e a m i l a k i ~ f i t e ~ k ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eviPJFoCGcJ6",
        "outputId": "cb2e4d90-46c3-4052-aa0d-9ec30f5f343b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(words, test_size=5000)\n",
        "\n",
        "print(\"Training on\", train.shape)\n",
        "print(\"Testing on\", test.shape)\n",
        "\n",
        "!mkdir -p data\n",
        "\n",
        "# Save as training\n",
        "# Can't use .to_csv because of quotins\n",
        "with open('data/src-train.txt', 'w') as f:\n",
        "    f.write('\\n'.join(train.trans))\n",
        "with open('data/tgt-train.txt', 'w') as f:\n",
        "    f.write('\\n'.join(train.orig))\n",
        "\n",
        "with open('data/src-val.txt', 'w') as f:\n",
        "    f.write('\\n'.join(test.trans))\n",
        "with open('data/tgt-val.txt', 'w') as f:\n",
        "    f.write('\\n'.join(test.orig))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on (92280, 2)\n",
            "Testing on (5000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4AFtbrgj2v2",
        "colab_type": "code",
        "outputId": "60e7b7eb-1c6f-415d-fec8-b7ed2f08da77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!head -n 5 data/tgt-val.txt"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ካ ል ተ ገ ኘ ~ ግ ን ~ ኢ ት ዮ ጵ ያ ~ በ ና ይ ል ~ ዱ ር ~ ድ ን በ ሯ ን ~ በ ዚ ህ ~ በ አ ዲ ስ ~ አ መ ት ~ እ ን ደ ም ታ ስ ከ ብ ር ~ ቃ ል ~ ገ ብ ተ ው ~ ሠ ራ ዊ ቱ ~ ለ ዚ ህ ~ እ ን ዲ ዘ ጋ ጅ ~ ህ ዝ ቡ ም ~ እ ን ደ ተ ለ መ ደ ው ~ ድ ጋ ፉ ን ~ እ ን ዲ ቀ ጥ ል ~ ጥ ሪ ~ አ ቅ ር በ ዋ ል ።\n",
            "የ ሮ ቤ ላ ው ያ ን ~ ቤ ተ ሰ ቦ ች ~ እ ነ ዚ ህ ~ ነ በ ሩ ፤ ~ ከ እ ነ ሱ ም ~ መ ካ ከ ል ~ የ ተ መ ዘ ገ ቡ ት ~ 4 3 , 7 3 0 ~ ነ በ ሩ ።\n",
            "በ ማ ሳ ሰ ቢ ያ ዎ ች ህ ~ ላ ይ ~ ስ ለ ማ ሰ ላ ስ ል ፣ ከ አ ስ ተ ማ ሪ ዎ ቼ ~ ሁ ሉ ~ የ በ ለ ጠ ~ ጥ ል ቅ ~ ማ ስ ተ ዋ ል ~ አ ለ ኝ ።\n",
            "አ ዶ ራ ይ ም ን ፣ ~ ለ ኪ ሶ ን ፣ ~ አ ዜ ቃ ን ፣\n",
            "ይ ህ ~ ሁ ኔ ታ ~ በ ሙ ስ ና ~ ከ ሥ ል ጣ ን ~ ተ ወ ገ ዱ ~ የ ሚ ባ ሉ ት ~ አ ቶ ~ ታ ም ራ ት ~ የ ታ ሠ ሩ ት ~ በ ኤ ር ት ራ ~ ጉ ዳ ይ ~ ላ ይ ~ በ ወ ሰ ዱ ት ~ አ ቋ ም ~ ነ ው ~ የ ሚ ለ ው ን ~ ጥ ር ጣ ሬ ~ ያ ጐ ላ ዋ ል ~ ይ ባ ላ ል ።\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SQVlDeyoGcJ-",
        "outputId": "fd7062c1-74a5-4fa6-a0ca-8d12e8dfd4c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>orig</th>\n",
              "      <th>trans</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>75372</th>\n",
              "      <td>ብ ዙ ዎ ች ~ “ አ ም ላ ክ ~ አ ያ ድ ነ ው ም ” ~ እ ያ ሉ ~ ...</td>\n",
              "      <td>b e z u w o c h i ~ \" a m l a k e ~ a y a d e ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52690</th>\n",
              "      <td>ሳ ኦ ል ም ~ ይ ሖ ዋ ~ ል ባ ቸ ው ን ~ ባ ነ ሳ ሳ ው ~ ተ ዋ ...</td>\n",
              "      <td>s a ' o l i m ~ y h o w a ~ l e b a c e w i n ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84785</th>\n",
              "      <td>እ ን ደ ~ እ ህ ል ~ የ ተ ወ ቃ ኸ ው ~ ሕ ዝ ቤ ፣ የ አ ው ድ ...</td>\n",
              "      <td>` e n d a ~ i h l i ~ j a t a w e k a k h a w ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35346</th>\n",
              "      <td>አ ቢ ያ ህ ~ ኢ ዮ ር ብ ዓ ም ን ~ አ ሳ ደ ደ ው ፤ ~ ከ ተ ሞ ...</td>\n",
              "      <td>` a b i y a h ~ i j o r b a m n e ~ a s a d e ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57668</th>\n",
              "      <td>ም ክ ን ያ ቱ ም ~ እ ነ ዚ ህ ~ ቀ ና ት ~ አ ይ ሁ ዳ ው ያ ኑ ...</td>\n",
              "      <td>m k n i j a t u m ~ ' e n a z i h ~ k a n a ' ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    orig                                              trans\n",
              "75372  ብ ዙ ዎ ች ~ “ አ ም ላ ክ ~ አ ያ ድ ነ ው ም ” ~ እ ያ ሉ ~ ...  b e z u w o c h i ~ \" a m l a k e ~ a y a d e ...\n",
              "52690  ሳ ኦ ል ም ~ ይ ሖ ዋ ~ ል ባ ቸ ው ን ~ ባ ነ ሳ ሳ ው ~ ተ ዋ ...  s a ' o l i m ~ y h o w a ~ l e b a c e w i n ...\n",
              "84785  እ ን ደ ~ እ ህ ል ~ የ ተ ወ ቃ ኸ ው ~ ሕ ዝ ቤ ፣ የ አ ው ድ ...  ` e n d a ~ i h l i ~ j a t a w e k a k h a w ...\n",
              "35346  አ ቢ ያ ህ ~ ኢ ዮ ር ብ ዓ ም ን ~ አ ሳ ደ ደ ው ፤ ~ ከ ተ ሞ ...  ` a b i y a h ~ i j o r b a m n e ~ a s a d e ...\n",
              "57668  ም ክ ን ያ ቱ ም ~ እ ነ ዚ ህ ~ ቀ ና ት ~ አ ይ ሁ ዳ ው ያ ኑ ...  m k n i j a t u m ~ ' e n a z i h ~ k a n a ' ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vddS6dk1GcKP"
      },
      "source": [
        "# Data setup\n",
        "\n",
        "Following quickstart instructions from https://github.com/OpenNMT/OpenNMT-py#quickstart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bNr6kIaNGcKQ"
      },
      "source": [
        "## Training\n",
        "\n",
        "I'm just using the terminal commands because the Python bindings were just Too Much Work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cd9tQuMjGcKR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1afd18df-d756-4c3b-e2cb-59e3873542d3"
      },
      "source": [
        "!pip install OpenNMT-py"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting OpenNMT-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/c7/b3d9bf9a6a681b10c00aa897650f79d4e7ad8a80317c5cddb6a3ef43540c/OpenNMT_py-1.1.1-py3-none-any.whl (189kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 26.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 20kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 30kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 40kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 51kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 61kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 71kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 81kB 10.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 92kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 102kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 112kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 122kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 133kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 143kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 153kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 163kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 174kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 184kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (0.16.0)\n",
            "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (2.2.1)\n",
            "Collecting pyonmttok==1.*; platform_system == \"Linux\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/20/3c57198ffe690b580fbf23d33d5000eb411862e60e4bb6853b61dc989187/pyonmttok-1.18.3-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 50.9MB/s \n",
            "\u001b[?25hCollecting configargparse\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/79/3045743bb26ca2e44a1d317c37395462bfed82dbbd38e69a3280b63696ce/ConfigArgParse-1.2.3.tar.gz (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n",
            "\u001b[?25hCollecting tqdm~=4.30.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/4c/103a4d3415dafc1ddfe6a6624333971756e2d3dd8c6dc0f520152855f040/tqdm-4.30.0-py2.py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.12.0)\n",
            "Collecting torchtext==0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.1.2)\n",
            "Collecting waitress\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/ca/ede3ed29723ca944f6e77bd1d7b38c271dd801c7d6a11ab6037597e4fd5b/waitress-1.4.3-py2.py3-none-any.whl (148kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 66.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (3.13)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.5.0+cu101)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (0.9.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (3.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (3.2.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (1.6.0.post3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (0.34.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (1.18.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (46.1.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (1.7.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (1.28.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask->OpenNMT-py) (2.11.2)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask->OpenNMT-py) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask->OpenNMT-py) (1.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py) (3.0.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py) (1.3.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py) (0.2.8)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask->OpenNMT-py) (1.1.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py) (0.4.8)\n",
            "Building wheels for collected packages: configargparse\n",
            "  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configargparse: filename=ConfigArgParse-1.2.3-cp36-none-any.whl size=19328 sha256=5868290a0f49d0b6b0e4b9c9aa7c5cd9110ad64ba20ee55c36a9068be28b75fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/d6/53/034032da9498bda2385cd50a51a289e88090b5da2d592b1fdf\n",
            "Successfully built configargparse\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.30.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pyonmttok, configargparse, tqdm, torchtext, waitress, OpenNMT-py\n",
            "  Found existing installation: tqdm 4.38.0\n",
            "    Uninstalling tqdm-4.38.0:\n",
            "      Successfully uninstalled tqdm-4.38.0\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed OpenNMT-py-1.1.1 configargparse-1.2.3 pyonmttok-1.18.3 torchtext-0.4.0 tqdm-4.30.0 waitress-1.4.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xhRO3mvhGcKV"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8mgNJrB8GcKW",
        "outputId": "a0e74225-ba5d-44b6-d017-5377edf269a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "!onmt_preprocess \\\n",
        "    -train_src data/src-train.txt \\\n",
        "    -train_tgt data/tgt-train.txt \\\n",
        "    -valid_src data/src-val.txt \\\n",
        "    -valid_tgt data/tgt-val.txt \\\n",
        "    -save_data data/demo \\\n",
        "    -overwrite"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2020-05-08 10:41:56,058 INFO] Extracting features...\n",
            "[2020-05-08 10:41:56,059 INFO]  * number of source features: 0.\n",
            "[2020-05-08 10:41:56,059 INFO]  * number of target features: 0.\n",
            "[2020-05-08 10:41:56,059 INFO] Building `Fields` object...\n",
            "[2020-05-08 10:41:56,059 INFO] Building & saving training data...\n",
            "[2020-05-08 10:41:56,059 WARNING] Shards for corpus train already exist, will be overwritten because `-overwrite` option is set.\n",
            "[2020-05-08 10:41:56,068 WARNING] Overwrite shards for corpus None\n",
            "[2020-05-08 10:41:56,450 INFO] Building shard 0.\n",
            "[2020-05-08 10:41:59,724 INFO]  * saving 0th train data shard to data/demo.train.0.pt.\n",
            "[2020-05-08 10:42:00,118 INFO]  * tgt vocab size: 327.\n",
            "[2020-05-08 10:42:00,118 INFO]  * src vocab size: 59.\n",
            "[2020-05-08 10:42:00,120 INFO] Building & saving validation data...\n",
            "[2020-05-08 10:42:00,120 WARNING] Shards for corpus valid already exist, will be overwritten because `-overwrite` option is set.\n",
            "[2020-05-08 10:42:00,125 WARNING] Overwrite shards for corpus None\n",
            "[2020-05-08 10:42:00,156 INFO] Building shard 0.\n",
            "[2020-05-08 10:42:00,271 INFO]  * saving 0th valid data shard to data/demo.valid.0.pt.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l8iBgYnFGcKb"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g1fZV5euGcKc",
        "outputId": "8c1125f4-5c2e-4350-99d5-3aa5023dd7c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Change to false to get GPU power on Colab\n",
        "if False:\n",
        "    !onmt_train \\\n",
        "        -data data/demo \\\n",
        "        -save_model demo-model \\\n",
        "        --valid_steps 50 \\\n",
        "        --train_steps 2 \\\n",
        "        --early_stopping 5\n",
        "else:\n",
        "    !CUDA_VISIBLE_DEVICES=0 \\\n",
        "        onmt_train \\\n",
        "        -encoder_type brnn \\\n",
        "        -world_size 1 \\\n",
        "        -gpu_ranks 0 \\\n",
        "        -data data/demo \\\n",
        "        -save_model demo-model \\\n",
        "        --valid_steps 1000 \\\n",
        "        --train_steps 5000 \\\n",
        "        --early_stopping 3"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2020-05-08 10:42:33,615 INFO]  * src vocab size = 59\n",
            "[2020-05-08 10:42:33,615 INFO]  * tgt vocab size = 327\n",
            "[2020-05-08 10:42:33,615 INFO] Building model...\n",
            "[2020-05-08 10:42:43,087 INFO] NMTModel(\n",
            "  (encoder): RNNEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(59, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (rnn): LSTM(500, 250, num_layers=2, dropout=0.3, bidirectional=True)\n",
            "  )\n",
            "  (decoder): InputFeedRNNDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(327, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (rnn): StackedLSTM(\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0): LSTMCell(1000, 500)\n",
            "        (1): LSTMCell(500, 500)\n",
            "      )\n",
            "    )\n",
            "    (attn): GlobalAttention(\n",
            "      (linear_in): Linear(in_features=500, out_features=500, bias=False)\n",
            "      (linear_out): Linear(in_features=1000, out_features=500, bias=False)\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=500, out_features=327, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax()\n",
            "  )\n",
            ")\n",
            "[2020-05-08 10:42:43,088 INFO] encoder: 3037500\n",
            "[2020-05-08 10:42:43,088 INFO] decoder: 6085327\n",
            "[2020-05-08 10:42:43,088 INFO] * number of parameters: 9122827\n",
            "[2020-05-08 10:42:43,089 INFO] Starting training on GPU: [0]\n",
            "[2020-05-08 10:42:43,089 INFO] Start training loop and validate every 1000 steps...\n",
            "[2020-05-08 10:42:43,089 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:42:43,127 INFO] number of examples: 6303\n",
            "[2020-05-08 10:42:46,534 INFO] Step 50/ 5000; acc:   8.64; ppl: 2717.45; xent: 7.91; lr: 1.00000; 32798/20139 tok/s;      3 sec\n",
            "[2020-05-08 10:42:49,311 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:42:49,383 INFO] number of examples: 6303\n",
            "[2020-05-08 10:42:49,468 INFO] Step 100/ 5000; acc:   8.10; ppl: 4416.99; xent: 8.39; lr: 1.00000; 34622/21490 tok/s;      6 sec\n",
            "[2020-05-08 10:42:52,600 INFO] Step 150/ 5000; acc:  10.08; ppl: 276.48; xent: 5.62; lr: 1.00000; 36226/22200 tok/s;     10 sec\n",
            "[2020-05-08 10:42:55,299 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:42:55,330 INFO] number of examples: 6303\n",
            "[2020-05-08 10:42:55,490 INFO] Step 200/ 5000; acc:  11.16; ppl: 152.41; xent: 5.03; lr: 1.00000; 35317/21910 tok/s;     12 sec\n",
            "[2020-05-08 10:42:58,572 INFO] Step 250/ 5000; acc:  13.41; ppl: 79.53; xent: 4.38; lr: 1.00000; 36434/22390 tok/s;     15 sec\n",
            "[2020-05-08 10:43:01,216 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:43:01,289 INFO] number of examples: 6303\n",
            "[2020-05-08 10:43:01,534 INFO] Step 300/ 5000; acc:  17.32; ppl: 57.45; xent: 4.05; lr: 1.00000; 34827/21582 tok/s;     18 sec\n",
            "[2020-05-08 10:43:04,600 INFO] Step 350/ 5000; acc:  21.01; ppl: 44.67; xent: 3.80; lr: 1.00000; 36482/22423 tok/s;     22 sec\n",
            "[2020-05-08 10:43:07,173 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:43:07,205 INFO] number of examples: 6303\n",
            "[2020-05-08 10:43:07,500 INFO] Step 400/ 5000; acc:  25.77; ppl: 32.48; xent: 3.48; lr: 1.00000; 35294/21881 tok/s;     24 sec\n",
            "[2020-05-08 10:43:10,558 INFO] Step 450/ 5000; acc:  28.38; ppl: 27.24; xent: 3.30; lr: 1.00000; 36248/22265 tok/s;     27 sec\n",
            "[2020-05-08 10:43:13,110 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:43:13,186 INFO] number of examples: 6303\n",
            "[2020-05-08 10:43:13,524 INFO] Step 500/ 5000; acc:  33.12; ppl: 20.28; xent: 3.01; lr: 1.00000; 34673/21499 tok/s;     30 sec\n",
            "[2020-05-08 10:43:16,550 INFO] Step 550/ 5000; acc:  35.92; ppl: 16.99; xent: 2.83; lr: 1.00000; 36679/22511 tok/s;     33 sec\n",
            "[2020-05-08 10:43:19,073 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:43:19,106 INFO] number of examples: 6303\n",
            "[2020-05-08 10:43:19,502 INFO] Step 600/ 5000; acc:  44.76; ppl: 10.77; xent: 2.38; lr: 1.00000; 35010/21726 tok/s;     36 sec\n",
            "[2020-05-08 10:43:22,521 INFO] Step 650/ 5000; acc:  54.42; ppl:  6.51; xent: 1.87; lr: 1.00000; 36619/22488 tok/s;     39 sec\n",
            "[2020-05-08 10:43:24,977 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:43:25,046 INFO] number of examples: 6303\n",
            "[2020-05-08 10:43:25,499 INFO] Step 700/ 5000; acc:  68.70; ppl:  3.28; xent: 1.19; lr: 1.00000; 35016/21689 tok/s;     42 sec\n",
            "[2020-05-08 10:43:28,506 INFO] Step 750/ 5000; acc:  76.35; ppl:  2.38; xent: 0.87; lr: 1.00000; 36413/22393 tok/s;     45 sec\n",
            "[2020-05-08 10:43:30,947 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:43:30,986 INFO] number of examples: 6303\n",
            "[2020-05-08 10:43:31,511 INFO] Step 800/ 5000; acc:  81.34; ppl:  1.90; xent: 0.64; lr: 1.00000; 35218/21766 tok/s;     48 sec\n",
            "[2020-05-08 10:43:34,480 INFO] Step 850/ 5000; acc:  83.98; ppl:  1.74; xent: 0.55; lr: 1.00000; 36342/22370 tok/s;     51 sec\n",
            "[2020-05-08 10:43:36,851 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:43:36,919 INFO] number of examples: 6303\n",
            "[2020-05-08 10:43:37,523 INFO] Step 900/ 5000; acc:  86.71; ppl:  1.56; xent: 0.45; lr: 1.00000; 35401/21864 tok/s;     54 sec\n",
            "[2020-05-08 10:43:40,484 INFO] Step 950/ 5000; acc:  87.74; ppl:  1.50; xent: 0.41; lr: 1.00000; 35774/22042 tok/s;     57 sec\n",
            "[2020-05-08 10:43:42,838 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:43:42,872 INFO] number of examples: 6303\n",
            "[2020-05-08 10:43:43,535 INFO] Step 1000/ 5000; acc:  89.60; ppl:  1.40; xent: 0.34; lr: 1.00000; 35572/22008 tok/s;     60 sec\n",
            "[2020-05-08 10:43:43,535 INFO] Loading dataset from data/demo.valid.0.pt\n",
            "[2020-05-08 10:43:43,658 INFO] number of examples: 5000\n",
            "[2020-05-08 10:44:10,394 INFO] Validation perplexity: 3.11813\n",
            "[2020-05-08 10:44:10,394 INFO] Validation accuracy: 73.5279\n",
            "[2020-05-08 10:44:10,394 INFO] Model is improving ppl: inf --> 3.11813.\n",
            "[2020-05-08 10:44:10,394 INFO] Model is improving acc: -inf --> 73.5279.\n",
            "[2020-05-08 10:44:13,361 INFO] Step 1050/ 5000; acc:  89.91; ppl:  1.39; xent: 0.33; lr: 1.00000; 3576/2195 tok/s;     90 sec\n",
            "[2020-05-08 10:44:15,662 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:44:15,754 INFO] number of examples: 6303\n",
            "[2020-05-08 10:44:16,502 INFO] Step 1100/ 5000; acc:  91.43; ppl:  1.32; xent: 0.28; lr: 1.00000; 34428/21349 tok/s;     93 sec\n",
            "[2020-05-08 10:44:19,405 INFO] Step 1150/ 5000; acc:  91.27; ppl:  1.32; xent: 0.28; lr: 1.00000; 36055/22130 tok/s;     96 sec\n",
            "[2020-05-08 10:44:21,689 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:44:21,723 INFO] number of examples: 6303\n",
            "[2020-05-08 10:44:22,489 INFO] Step 1200/ 5000; acc:  92.44; ppl:  1.28; xent: 0.24; lr: 1.00000; 35146/21792 tok/s;     99 sec\n",
            "[2020-05-08 10:44:25,413 INFO] Step 1250/ 5000; acc:  92.49; ppl:  1.27; xent: 0.24; lr: 1.00000; 36539/22429 tok/s;    102 sec\n",
            "[2020-05-08 10:44:27,605 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:44:27,694 INFO] number of examples: 6303\n",
            "[2020-05-08 10:44:28,509 INFO] Step 1300/ 5000; acc:  93.61; ppl:  1.23; xent: 0.20; lr: 1.00000; 34811/21552 tok/s;    105 sec\n",
            "[2020-05-08 10:44:31,428 INFO] Step 1350/ 5000; acc:  93.62; ppl:  1.23; xent: 0.21; lr: 1.00000; 36196/22250 tok/s;    108 sec\n",
            "[2020-05-08 10:44:33,568 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:44:33,602 INFO] number of examples: 6303\n",
            "[2020-05-08 10:44:34,462 INFO] Step 1400/ 5000; acc:  94.26; ppl:  1.20; xent: 0.18; lr: 1.00000; 35576/22018 tok/s;    111 sec\n",
            "[2020-05-08 10:44:37,389 INFO] Step 1450/ 5000; acc:  94.14; ppl:  1.21; xent: 0.19; lr: 1.00000; 36223/22299 tok/s;    114 sec\n",
            "[2020-05-08 10:44:39,491 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:44:39,577 INFO] number of examples: 6303\n",
            "[2020-05-08 10:44:40,520 INFO] Step 1500/ 5000; acc:  94.87; ppl:  1.18; xent: 0.17; lr: 1.00000; 34948/21589 tok/s;    117 sec\n",
            "[2020-05-08 10:44:43,483 INFO] Step 1550/ 5000; acc:  94.66; ppl:  1.19; xent: 0.17; lr: 1.00000; 35806/22045 tok/s;    120 sec\n",
            "[2020-05-08 10:44:45,509 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:44:45,544 INFO] number of examples: 6303\n",
            "[2020-05-08 10:44:46,546 INFO] Step 1600/ 5000; acc:  95.42; ppl:  1.17; xent: 0.15; lr: 1.00000; 35598/21984 tok/s;    123 sec\n",
            "[2020-05-08 10:44:49,423 INFO] Step 1650/ 5000; acc:  95.02; ppl:  1.18; xent: 0.17; lr: 1.00000; 36219/22331 tok/s;    126 sec\n",
            "[2020-05-08 10:44:51,448 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:44:51,541 INFO] number of examples: 6303\n",
            "[2020-05-08 10:44:52,595 INFO] Step 1700/ 5000; acc:  95.73; ppl:  1.15; xent: 0.14; lr: 1.00000; 34457/21270 tok/s;    130 sec\n",
            "[2020-05-08 10:44:55,503 INFO] Step 1750/ 5000; acc:  95.36; ppl:  1.16; xent: 0.15; lr: 1.00000; 36225/22335 tok/s;    132 sec\n",
            "[2020-05-08 10:44:57,444 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:44:57,479 INFO] number of examples: 6303\n",
            "[2020-05-08 10:44:58,580 INFO] Step 1800/ 5000; acc:  96.05; ppl:  1.14; xent: 0.13; lr: 1.00000; 35447/21876 tok/s;    135 sec\n",
            "[2020-05-08 10:45:01,507 INFO] Step 1850/ 5000; acc:  95.77; ppl:  1.15; xent: 0.14; lr: 1.00000; 36034/22187 tok/s;    138 sec\n",
            "[2020-05-08 10:45:03,421 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:45:03,511 INFO] number of examples: 6303\n",
            "[2020-05-08 10:45:04,648 INFO] Step 1900/ 5000; acc:  96.25; ppl:  1.13; xent: 0.13; lr: 1.00000; 34363/21240 tok/s;    142 sec\n",
            "[2020-05-08 10:45:07,589 INFO] Step 1950/ 5000; acc:  95.92; ppl:  1.15; xent: 0.14; lr: 1.00000; 36086/22213 tok/s;    144 sec\n",
            "[2020-05-08 10:45:09,449 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:45:09,484 INFO] number of examples: 6303\n",
            "[2020-05-08 10:45:10,708 INFO] Step 2000/ 5000; acc:  96.36; ppl:  1.13; xent: 0.12; lr: 1.00000; 35073/21694 tok/s;    148 sec\n",
            "[2020-05-08 10:45:10,708 INFO] Loading dataset from data/demo.valid.0.pt\n",
            "[2020-05-08 10:45:10,850 INFO] number of examples: 5000\n",
            "[2020-05-08 10:45:37,242 INFO] Validation perplexity: 3.05337\n",
            "[2020-05-08 10:45:37,243 INFO] Validation accuracy: 79.6915\n",
            "[2020-05-08 10:45:37,243 INFO] Model is improving ppl: 3.11813 --> 3.05337.\n",
            "[2020-05-08 10:45:37,243 INFO] Model is improving acc: 73.5279 --> 79.6915.\n",
            "[2020-05-08 10:45:40,151 INFO] Step 2050/ 5000; acc:  96.27; ppl:  1.13; xent: 0.12; lr: 1.00000; 3578/2203 tok/s;    177 sec\n",
            "[2020-05-08 10:45:41,924 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:45:41,958 INFO] number of examples: 6303\n",
            "[2020-05-08 10:45:43,280 INFO] Step 2100/ 5000; acc:  96.48; ppl:  1.13; xent: 0.12; lr: 1.00000; 34874/21557 tok/s;    180 sec\n",
            "[2020-05-08 10:45:46,240 INFO] Step 2150/ 5000; acc:  96.41; ppl:  1.13; xent: 0.13; lr: 1.00000; 35940/22096 tok/s;    183 sec\n",
            "[2020-05-08 10:45:47,953 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:45:47,988 INFO] number of examples: 6303\n",
            "[2020-05-08 10:45:49,295 INFO] Step 2200/ 5000; acc:  96.75; ppl:  1.11; xent: 0.11; lr: 1.00000; 35261/21811 tok/s;    186 sec\n",
            "[2020-05-08 10:45:52,293 INFO] Step 2250/ 5000; acc:  96.49; ppl:  1.12; xent: 0.12; lr: 1.00000; 35858/22023 tok/s;    189 sec\n",
            "[2020-05-08 10:45:53,928 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:45:54,017 INFO] number of examples: 6303\n",
            "[2020-05-08 10:45:55,393 INFO] Step 2300/ 5000; acc:  96.78; ppl:  1.13; xent: 0.12; lr: 1.00000; 34577/21396 tok/s;    192 sec\n",
            "[2020-05-08 10:45:58,330 INFO] Step 2350/ 5000; acc:  96.52; ppl:  1.13; xent: 0.12; lr: 1.00000; 36220/22278 tok/s;    195 sec\n",
            "[2020-05-08 10:45:59,914 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:45:59,949 INFO] number of examples: 6303\n",
            "[2020-05-08 10:46:01,377 INFO] Step 2400/ 5000; acc:  96.77; ppl:  1.12; xent: 0.11; lr: 1.00000; 35840/22120 tok/s;    198 sec\n",
            "[2020-05-08 10:46:04,307 INFO] Step 2450/ 5000; acc:  96.70; ppl:  1.12; xent: 0.11; lr: 1.00000; 35933/22196 tok/s;    201 sec\n",
            "[2020-05-08 10:46:05,853 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:46:05,940 INFO] number of examples: 6303\n",
            "[2020-05-08 10:46:07,444 INFO] Step 2500/ 5000; acc:  96.91; ppl:  1.11; xent: 0.10; lr: 1.00000; 35106/21623 tok/s;    204 sec\n",
            "[2020-05-08 10:46:10,356 INFO] Step 2550/ 5000; acc:  96.99; ppl:  1.11; xent: 0.10; lr: 1.00000; 35967/22213 tok/s;    207 sec\n",
            "[2020-05-08 10:46:11,841 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:46:11,876 INFO] number of examples: 6303\n",
            "[2020-05-08 10:46:13,468 INFO] Step 2600/ 5000; acc:  96.91; ppl:  1.11; xent: 0.10; lr: 1.00000; 35637/21958 tok/s;    210 sec\n",
            "[2020-05-08 10:46:16,371 INFO] Step 2650/ 5000; acc:  97.12; ppl:  1.10; xent: 0.10; lr: 1.00000; 35537/21984 tok/s;    213 sec\n",
            "[2020-05-08 10:46:17,827 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:46:17,915 INFO] number of examples: 6303\n",
            "[2020-05-08 10:46:19,593 INFO] Step 2700/ 5000; acc:  96.98; ppl:  1.11; xent: 0.10; lr: 1.00000; 34735/21335 tok/s;    217 sec\n",
            "[2020-05-08 10:46:22,502 INFO] Step 2750/ 5000; acc:  97.32; ppl:  1.10; xent: 0.09; lr: 1.00000; 35196/21812 tok/s;    219 sec\n",
            "[2020-05-08 10:46:23,889 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:46:23,924 INFO] number of examples: 6303\n",
            "[2020-05-08 10:46:25,685 INFO] Step 2800/ 5000; acc:  97.10; ppl:  1.11; xent: 0.10; lr: 1.00000; 35178/21587 tok/s;    223 sec\n",
            "[2020-05-08 10:46:28,570 INFO] Step 2850/ 5000; acc:  97.27; ppl:  1.10; xent: 0.09; lr: 1.00000; 35516/22005 tok/s;    225 sec\n",
            "[2020-05-08 10:46:29,895 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:46:29,991 INFO] number of examples: 6303\n",
            "[2020-05-08 10:46:31,798 INFO] Step 2900/ 5000; acc:  96.99; ppl:  1.11; xent: 0.10; lr: 1.00000; 35041/21532 tok/s;    229 sec\n",
            "[2020-05-08 10:46:34,618 INFO] Step 2950/ 5000; acc:  97.29; ppl:  1.10; xent: 0.10; lr: 1.00000; 35402/21938 tok/s;    232 sec\n",
            "[2020-05-08 10:46:35,917 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:46:35,951 INFO] number of examples: 6303\n",
            "[2020-05-08 10:46:37,795 INFO] Step 3000/ 5000; acc:  97.13; ppl:  1.10; xent: 0.10; lr: 1.00000; 35930/22080 tok/s;    235 sec\n",
            "[2020-05-08 10:46:37,795 INFO] Loading dataset from data/demo.valid.0.pt\n",
            "[2020-05-08 10:46:37,936 INFO] number of examples: 5000\n",
            "[2020-05-08 10:47:04,529 INFO] Validation perplexity: 3.52412\n",
            "[2020-05-08 10:47:04,529 INFO] Validation accuracy: 79.1077\n",
            "[2020-05-08 10:47:04,529 INFO] Decreasing patience: 2/3\n",
            "[2020-05-08 10:47:07,370 INFO] Step 3050/ 5000; acc:  97.37; ppl:  1.10; xent: 0.09; lr: 1.00000; 3407/2110 tok/s;    264 sec\n",
            "[2020-05-08 10:47:08,593 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:47:08,627 INFO] number of examples: 6303\n",
            "[2020-05-08 10:47:10,543 INFO] Step 3100/ 5000; acc:  97.08; ppl:  1.10; xent: 0.10; lr: 1.00000; 36155/22208 tok/s;    267 sec\n",
            "[2020-05-08 10:47:13,363 INFO] Step 3150/ 5000; acc:  97.37; ppl:  1.10; xent: 0.09; lr: 1.00000; 35634/22066 tok/s;    270 sec\n",
            "[2020-05-08 10:47:14,526 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:47:14,611 INFO] number of examples: 6303\n",
            "[2020-05-08 10:47:16,601 INFO] Step 3200/ 5000; acc:  97.31; ppl:  1.09; xent: 0.09; lr: 1.00000; 35348/21706 tok/s;    274 sec\n",
            "[2020-05-08 10:47:19,426 INFO] Step 3250/ 5000; acc:  97.38; ppl:  1.10; xent: 0.09; lr: 1.00000; 35587/22044 tok/s;    276 sec\n",
            "[2020-05-08 10:47:20,528 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:47:20,561 INFO] number of examples: 6303\n",
            "[2020-05-08 10:47:22,687 INFO] Step 3300/ 5000; acc:  97.34; ppl:  1.09; xent: 0.09; lr: 1.00000; 35180/21628 tok/s;    280 sec\n",
            "[2020-05-08 10:47:25,507 INFO] Step 3350/ 5000; acc:  97.46; ppl:  1.10; xent: 0.09; lr: 1.00000; 35432/21966 tok/s;    282 sec\n",
            "[2020-05-08 10:47:26,536 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:47:26,572 INFO] number of examples: 6303\n",
            "[2020-05-08 10:47:28,711 INFO] Step 3400/ 5000; acc:  97.19; ppl:  1.11; xent: 0.10; lr: 1.00000; 36000/22093 tok/s;    286 sec\n",
            "[2020-05-08 10:47:31,521 INFO] Step 3450/ 5000; acc:  97.32; ppl:  1.10; xent: 0.09; lr: 1.00000; 35583/22049 tok/s;    288 sec\n",
            "[2020-05-08 10:47:32,471 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:47:32,505 INFO] number of examples: 6303\n",
            "[2020-05-08 10:47:34,733 INFO] Step 3500/ 5000; acc:  97.24; ppl:  1.10; xent: 0.10; lr: 1.00000; 35554/21848 tok/s;    292 sec\n",
            "[2020-05-08 10:47:37,584 INFO] Step 3550/ 5000; acc:  97.53; ppl:  1.09; xent: 0.09; lr: 1.00000; 35568/22014 tok/s;    294 sec\n",
            "[2020-05-08 10:47:38,463 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:47:38,497 INFO] number of examples: 6303\n",
            "[2020-05-08 10:47:40,723 INFO] Step 3600/ 5000; acc:  97.34; ppl:  1.10; xent: 0.09; lr: 1.00000; 35969/22103 tok/s;    298 sec\n",
            "[2020-05-08 10:47:43,600 INFO] Step 3650/ 5000; acc:  97.52; ppl:  1.09; xent: 0.09; lr: 1.00000; 35423/21964 tok/s;    301 sec\n",
            "[2020-05-08 10:47:44,418 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:47:44,511 INFO] number of examples: 6303\n",
            "[2020-05-08 10:47:46,800 INFO] Step 3700/ 5000; acc:  97.31; ppl:  1.10; xent: 0.09; lr: 1.00000; 34950/21459 tok/s;    304 sec\n",
            "[2020-05-08 10:47:49,718 INFO] Step 3750/ 5000; acc:  97.41; ppl:  1.10; xent: 0.09; lr: 1.00000; 35473/21975 tok/s;    307 sec\n",
            "[2020-05-08 10:47:50,455 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:47:50,490 INFO] number of examples: 6303\n",
            "[2020-05-08 10:47:52,879 INFO] Step 3800/ 5000; acc:  97.28; ppl:  1.10; xent: 0.10; lr: 1.00000; 35179/21630 tok/s;    310 sec\n",
            "[2020-05-08 10:47:55,749 INFO] Step 3850/ 5000; acc:  97.46; ppl:  1.10; xent: 0.09; lr: 1.00000; 35920/22276 tok/s;    313 sec\n",
            "[2020-05-08 10:47:56,434 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:47:56,531 INFO] number of examples: 6303\n",
            "[2020-05-08 10:47:58,959 INFO] Step 3900/ 5000; acc:  97.42; ppl:  1.09; xent: 0.09; lr: 1.00000; 34909/21440 tok/s;    316 sec\n",
            "[2020-05-08 10:48:01,835 INFO] Step 3950/ 5000; acc:  97.55; ppl:  1.09; xent: 0.09; lr: 1.00000; 35821/22218 tok/s;    319 sec\n",
            "[2020-05-08 10:48:02,445 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:48:02,480 INFO] number of examples: 6303\n",
            "[2020-05-08 10:48:04,970 INFO] Step 4000/ 5000; acc:  97.45; ppl:  1.10; xent: 0.09; lr: 1.00000; 35920/22017 tok/s;    322 sec\n",
            "[2020-05-08 10:48:04,970 INFO] Loading dataset from data/demo.valid.0.pt\n",
            "[2020-05-08 10:48:05,119 INFO] number of examples: 5000\n",
            "[2020-05-08 10:48:31,587 INFO] Validation perplexity: 5.11949\n",
            "[2020-05-08 10:48:31,587 INFO] Validation accuracy: 77.7201\n",
            "[2020-05-08 10:48:31,588 INFO] Decreasing patience: 1/3\n",
            "[2020-05-08 10:48:34,458 INFO] Step 4050/ 5000; acc:  97.45; ppl:  1.10; xent: 0.09; lr: 1.00000; 3477/2156 tok/s;    351 sec\n",
            "[2020-05-08 10:48:34,988 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:48:35,023 INFO] number of examples: 6303\n",
            "[2020-05-08 10:48:37,565 INFO] Step 4100/ 5000; acc:  97.39; ppl:  1.10; xent: 0.10; lr: 1.00000; 35932/22054 tok/s;    354 sec\n",
            "[2020-05-08 10:48:40,460 INFO] Step 4150/ 5000; acc:  97.36; ppl:  1.10; xent: 0.10; lr: 1.00000; 35659/22083 tok/s;    357 sec\n",
            "[2020-05-08 10:48:40,914 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:48:41,001 INFO] number of examples: 6303\n",
            "[2020-05-08 10:48:43,618 INFO] Step 4200/ 5000; acc:  97.23; ppl:  1.10; xent: 0.10; lr: 1.00000; 35153/21602 tok/s;    361 sec\n",
            "[2020-05-08 10:48:46,505 INFO] Step 4250/ 5000; acc:  97.52; ppl:  1.09; xent: 0.09; lr: 1.00000; 35936/22232 tok/s;    363 sec\n",
            "[2020-05-08 10:48:46,900 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:48:46,935 INFO] number of examples: 6303\n",
            "[2020-05-08 10:48:49,611 INFO] Step 4300/ 5000; acc:  97.41; ppl:  1.10; xent: 0.09; lr: 1.00000; 35489/21845 tok/s;    367 sec\n",
            "[2020-05-08 10:48:52,555 INFO] Step 4350/ 5000; acc:  97.43; ppl:  1.10; xent: 0.09; lr: 1.00000; 35804/22091 tok/s;    369 sec\n",
            "[2020-05-08 10:48:52,876 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:48:52,958 INFO] number of examples: 6303\n",
            "[2020-05-08 10:48:55,700 INFO] Step 4400/ 5000; acc:  97.36; ppl:  1.10; xent: 0.09; lr: 1.00000; 34711/21407 tok/s;    373 sec\n",
            "[2020-05-08 10:48:58,572 INFO] Step 4450/ 5000; acc:  97.35; ppl:  1.10; xent: 0.10; lr: 1.00000; 36272/22394 tok/s;    375 sec\n",
            "[2020-05-08 10:48:58,862 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:48:58,897 INFO] number of examples: 6303\n",
            "[2020-05-08 10:49:01,723 INFO] Step 4500/ 5000; acc:  97.28; ppl:  1.11; xent: 0.10; lr: 1.00000; 35277/21744 tok/s;    379 sec\n",
            "[2020-05-08 10:49:04,603 INFO] Step 4550/ 5000; acc:  97.50; ppl:  1.10; xent: 0.09; lr: 1.00000; 35947/22193 tok/s;    382 sec\n",
            "[2020-05-08 10:49:04,832 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:49:04,925 INFO] number of examples: 6303\n",
            "[2020-05-08 10:49:07,794 INFO] Step 4600/ 5000; acc:  97.05; ppl:  1.12; xent: 0.11; lr: 1.00000; 34590/21327 tok/s;    385 sec\n",
            "[2020-05-08 10:49:10,736 INFO] Step 4650/ 5000; acc:  97.39; ppl:  1.10; xent: 0.09; lr: 1.00000; 35222/21773 tok/s;    388 sec\n",
            "[2020-05-08 10:49:10,921 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:49:10,964 INFO] number of examples: 6303\n",
            "[2020-05-08 10:49:13,987 INFO] Step 4700/ 5000; acc:  97.28; ppl:  1.10; xent: 0.10; lr: 1.00000; 34282/21092 tok/s;    391 sec\n",
            "[2020-05-08 10:49:16,887 INFO] Step 4750/ 5000; acc:  97.50; ppl:  1.09; xent: 0.09; lr: 1.00000; 35530/21963 tok/s;    394 sec\n",
            "[2020-05-08 10:49:17,004 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:49:17,038 INFO] number of examples: 6303\n",
            "[2020-05-08 10:49:20,033 INFO] Step 4800/ 5000; acc:  97.42; ppl:  1.10; xent: 0.10; lr: 1.00000; 35350/21801 tok/s;    397 sec\n",
            "[2020-05-08 10:49:22,926 INFO] Step 4850/ 5000; acc:  97.48; ppl:  1.10; xent: 0.10; lr: 1.00000; 35857/22129 tok/s;    400 sec\n",
            "[2020-05-08 10:49:22,976 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:49:23,066 INFO] number of examples: 6303\n",
            "[2020-05-08 10:49:26,123 INFO] Step 4900/ 5000; acc:  97.21; ppl:  1.11; xent: 0.10; lr: 1.00000; 34928/21510 tok/s;    403 sec\n",
            "[2020-05-08 10:49:28,971 INFO] Step 4950/ 5000; acc:  97.46; ppl:  1.10; xent: 0.10; lr: 1.00000; 35969/22262 tok/s;    406 sec\n",
            "[2020-05-08 10:49:28,971 INFO] Loading dataset from data/demo.train.0.pt\n",
            "[2020-05-08 10:49:29,006 INFO] number of examples: 6303\n",
            "[2020-05-08 10:49:32,130 INFO] Step 5000/ 5000; acc:  97.28; ppl:  1.11; xent: 0.10; lr: 1.00000; 35770/21963 tok/s;    409 sec\n",
            "[2020-05-08 10:49:32,131 INFO] Loading dataset from data/demo.valid.0.pt\n",
            "[2020-05-08 10:49:32,211 INFO] number of examples: 5000\n",
            "[2020-05-08 10:49:58,877 INFO] Validation perplexity: 5.12733\n",
            "[2020-05-08 10:49:58,877 INFO] Validation accuracy: 79.8129\n",
            "[2020-05-08 10:49:58,877 INFO] Stalled patience: 2/3\n",
            "[2020-05-08 10:49:58,879 INFO] Saving checkpoint demo-model_step_5000.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBlLlNWXnDGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import subprocess\n",
        "\n",
        "# Pull the first 10 originals and transliterateds\n",
        "originals = subprocess.run(\"head -n 500 original.txt\",\n",
        "                            shell=True,\n",
        "                            stdout=subprocess.PIPE).stdout.decode(\"utf-8\").strip()\n",
        "transliterated = subprocess.run(\"head -n 500 transliterated.txt\",\n",
        "                            shell=True,\n",
        "                            stdout=subprocess.PIPE).stdout.decode(\"utf-8\").strip()\n",
        "\n",
        "# You can also use other stuff\n",
        "#originals = open(\"sera_am.txt\").read()\n",
        "#transliterated = open(\"sera_rom.txt\").read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2IUHqEKxMGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install unidecode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZi_t8FLqMjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import unidecode\n",
        "\n",
        "def clean(lat_word):\n",
        "  return unidecode.unidecode(lat_word).lower()\n",
        "\n",
        "orig_test = [encode_sentence(sent).strip() for sent in originals.splitlines()]\n",
        "trans_test = [encode_sentence(clean(sent)).strip() for sent in transliterated.splitlines()]\n",
        "orig_test = [sent for sent in orig_test if sent]\n",
        "trans_test = [sent for sent in trans_test if sent]\n",
        "if(len(orig_test) != len(trans_test)):\n",
        "  print(\"Unequal lines, won't be able to compare!!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6wYh0Zi5GcKh",
        "colab": {}
      },
      "source": [
        "# import re\n",
        "\n",
        "# orig_test = [' '.join(word) for word in re.split('\\s+', originals)]\n",
        "# trans_test = [' '.join(word) for word in re.split('\\s+', transliterated)]\n",
        "with open(\"data/test.txt\", 'w') as f:\n",
        "    f.write('\\n'.join(trans_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvY_F7AnukLR",
        "colab_type": "code",
        "outputId": "d7455397-089b-473a-c79f-9691a611d64c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "!head -n 10 data/test.txt"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "k a z e q a z a ~ w e h a ~ y a z a l a t c h e n e ~ n e f s i ~ i n i d a m i y a r a k a ~ h u l u k e r u q e ~ a g a r ~ j a m e t a ~ m e l e k a m ~ w e r e m ~ i n e d i h u ~ n e w .\n",
            "b e m a l k a m ~ h u n e t a ~ j a m i y a s i t a d a d e r u ~ b a t e l a y e ~ d a g i m o ~ b a m a n a ' g e r n a ~ b e m a s t e m a r i ~ t a g e t a w i ~ j a m i s a r u ~ x m a g e l e w o c e ~ i t f e ~ k e b i r e ~ l i s e t a c a w ~ y i g e b a l e .\n",
            "m a s h e f e ~ q d u s e ~ b e m e c e r a x a w o t c h u ~ q a n a ' t e ~ k e f a t ~ b a k a f i t a g n a ~ h u n y e t a ~ ' e n e d e m i b a b a s ~ y n e g r a n a ' l e .\n",
            "y e ' a k e s u m ~ h a w l t e ~ h u l e t a n a w ~ k f a y e ~ a k i s u m e ~ g e b a\n",
            "y a s y e m e ~ w a n i d w o c e ~ l g o c e ~ ' e l a m , ~ ` a s h u r e , ~ ` a r i f a k s i d e , ~ l u d i ~ i n a ~ ' a r a m e ~ n e b a r u .\n",
            "l a m i m a r e m e r u n ~ y a m a q e r e b e w i ~ y a m a k a l a k a y a ~ m a l s i ~ j e h ~ n a w :\n",
            "y a l a n a q e f a ~ y a m i m a l a l e s , t k i k l ~ y e h o n e w e n e ~ j a m i y a d a r g i , b e l e b u m ~ ` e w n a t n ~ y e m i n a ' g e r ~ s e w e ~ n e w e .\n",
            "g e n a ~ s a y i t a r u ~ i m a l i s e l a c a w a l a h u ; ' e y a t a n a g e r u ~ s a l u ~ ' e s e m a t c h a w a l e h u .\n",
            "b a z i h ~ g i z e ~ d a q e ~ m e z a m u r e t u ~ \" g e t a ~ h w o y e , ~ t a n i t o ~ k e h w o n a ~ y e s h a l a w a l i \" ~ a l u t .\n",
            "l i b i h e ~ b e a m i l a k i ~ f i t e ~ k a n a ~ s l a l i h w o n e ~ b a z i h i ~ ` a g a l i g l w o t e ~ m n m i ~ a j e n a t ~ d e r e c h a m ~ h o n e ~ i d l e ~ f a n t a ~ j a l e h e m i .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVpJmHhJilhk",
        "colab_type": "code",
        "outputId": "9686581c-c2fc-4b11-fbff-18e6ba56e8e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(len(trans_test), trans_test[:5])\n",
        "print(len(orig_test), orig_test[:5])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500 ['k a z e q a z a ~ w e h a ~ y a z a l a t c h e n e ~ n e f s i ~ i n i d a m i y a r a k a ~ h u l u k e r u q e ~ a g a r ~ j a m e t a ~ m e l e k a m ~ w e r e m ~ i n e d i h u ~ n e w .', \"b e m a l k a m ~ h u n e t a ~ j a m i y a s i t a d a d e r u ~ b a t e l a y e ~ d a g i m o ~ b a m a n a ' g e r n a ~ b e m a s t e m a r i ~ t a g e t a w i ~ j a m i s a r u ~ x m a g e l e w o c e ~ i t f e ~ k e b i r e ~ l i s e t a c a w ~ y i g e b a l e .\", \"m a s h e f e ~ q d u s e ~ b e m e c e r a x a w o t c h u ~ q a n a ' t e ~ k e f a t ~ b a k a f i t a g n a ~ h u n y e t a ~ ' e n e d e m i b a b a s ~ y n e g r a n a ' l e .\", \"y e ' a k e s u m ~ h a w l t e ~ h u l e t a n a w ~ k f a y e ~ a k i s u m e ~ g e b a\", \"y a s y e m e ~ w a n i d w o c e ~ l g o c e ~ ' e l a m , ~ ` a s h u r e , ~ ` a r i f a k s i d e , ~ l u d i ~ i n a ~ ' a r a m e ~ n e b a r u .\"]\n",
            "500 ['ቀ ዝ ቃ ዛ ~ ው ኃ ~ የ ዛ ለ ች ን ~ ነ ፍ ስ ~ እ ን ደ ሚ ያ ረ ካ ~ ሁ ሉ ከ ሩ ቅ ~ አ ገ ር ~ የ መ ጣ ~ መ ል ካ ም ~ ወ ሬ ም ~ እ ን ዲ ሁ ~ ነ ው ።', 'በ መ ል ካ ም ~ ሁ ኔ ታ ~ የ ሚ ያ ስ ተ ዳ ድ ሩ ~ በ ተ ለ ይ ~ ደ ግ ሞ ~ በ መ ና ገ ር ና ~ በ ማ ስ ተ ማ ር ~ ተ ግ ተ ው ~ የ ሚ ሠ ሩ ~ ሽ ማ ግ ሌ ዎ ች ~ እ ጥ ፍ ~ ክ ብ ር ~ ሊ ሰ ጣ ቸ ው ~ ይ ገ ባ ል ።', 'መ ጽ ሐ ፍ ~ ቅ ዱ ስ ~ በ መ ጨ ረ ሻ ዎ ቹ ~ ቀ ና ት ~ ክ ፋ ት ~ በ ከ ፍ ተ ኛ ~ ሁ ኔ ታ ~ እ ን ደ ሚ ባ ባ ስ ~ ይ ነ ግ ረ ና ል ።', 'የ አ ክ ሱ ም ~ ሐ ው ል ት ~ ሁ ለ ተ ኛ ው ~ ክ ፋ ይ ~ አ ክ ሱ ም ~ ገ ባ', 'የ ሴ ም ~ ወ ን ዶ ች ~ ል ጆ ች ~ ኤ ላ ም ፣ ~ አ ሹ ር ፣ ~ አ ር ፋ ክ ስ ድ ፣ ~ ሉ ድ ~ እ ና ~ አ ራ ም ~ ነ በ ሩ ።']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kL7FyKlBGcKk"
      },
      "source": [
        "**You'll need to change the model name in `onmt_translate` below.** It's probably the most recently changed model file, so at the top of this list: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EkD1GQUeGcKk",
        "outputId": "d313fdb1-b796-42a7-f84c-c15970fd4a9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_name = subprocess.run(\"ls -t *model* | head -n 1\",\n",
        "                            shell=True,\n",
        "                            stdout=subprocess.PIPE).stdout.decode(\"utf-8\").strip()\n",
        "print(\"Using model\", model_name)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using model demo-model_step_5000.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FFRDPiz6GcKn",
        "outputId": "80d7b03e-1535-4241-9382-019c604e4c9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "!onmt_translate \\\n",
        "    -model {model_name} \\\n",
        "    -src data/test.txt \\\n",
        "    -output data/pred.txt -replace_unk\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2020-05-08 10:53:38,866 INFO] Translating shard 0.\n",
            "/pytorch/aten/src/ATen/native/BinaryOps.cpp:66: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n",
            "PRED AVG SCORE: -0.0549, PRED PPL: 1.0565\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVGrVHI07uMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentences_to_words(sentences):\n",
        "  decoded = [decode_sentence(s) for s in sentences]\n",
        "  return ' '.join(decoded).split(' ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUjCpmYA7uSH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d540373d-e535-4229-f1d6-ed10ecb95f79"
      },
      "source": [
        "len(sentences_to_words(trans_test))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7650"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Aw_FJI7GcKu",
        "outputId": "317e29d4-3df1-473a-9264-d20db8703dda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        }
      },
      "source": [
        "results = open(\"data/pred.txt\").read().splitlines()\n",
        "test_results = pd.DataFrame({\n",
        "    'result': results,\n",
        "    'original': orig_test,\n",
        "    'transliterated': trans_test\n",
        "})\n",
        "\n",
        "test_results.result = test_results.result.apply(decode_sentence)\n",
        "test_results.original = test_results.original.apply(decode_sentence)\n",
        "test_results.transliterated = test_results.transliterated.apply(decode_sentence)\n",
        "\n",
        "test_results['is_correct'] = test_results.result == test_results.original\n",
        "print(test_results.is_correct.value_counts(normalize=True))\n",
        "test_results"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False    0.92\n",
            "True     0.08\n",
            "Name: is_correct, dtype: float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>result</th>\n",
              "      <th>original</th>\n",
              "      <th>transliterated</th>\n",
              "      <th>is_correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ከዘቃዘ ውሃ የዛላችን ንፍስ እንደሚያረከ ሁሉክሩቅ አገር የመጣ መልካም ወ...</td>\n",
              "      <td>ቀዝቃዛ ውኃ የዛለችን ነፍስ እንደሚያረካ ሁሉከሩቅ አገር የመጣ መልካም ወ...</td>\n",
              "      <td>kazeqaza weha yazalatchene nefsi inidamiyaraka...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>በማልካም ሁኔታ የሚያስተማር ተገጣው ይገባል።</td>\n",
              "      <td>በመልካም ሁኔታ የሚያስተዳድሩ በተለይ ደግሞ በመናገርና በማስተማር ተግተው...</td>\n",
              "      <td>bemalkam huneta jamiyasitadaderu batelaye dagi...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>መጽሐፍ ቅዱስ በመጨረሻቹ ቀናቴ በካፍተኛ ሁኔታ እንደምበባስ ይነግራናል።</td>\n",
              "      <td>መጽሐፍ ቅዱስ በመጨረሻዎቹ ቀናት ክፋት በከፍተኛ ሁኔታ እንደሚባባስ ይነግ...</td>\n",
              "      <td>mashefe qduse bemeceraxawotchu qana'te kefat b...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>የአክሱም ሐውልት ሁለተኛው ክፋይ አቅሱም ገባ</td>\n",
              "      <td>የአክሱም ሐውልት ሁለተኛው ክፋይ አክሱም ገባ</td>\n",
              "      <td>ye'akesum hawlte huletanaw kfaye akisume geba</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ያሴም ወንዶች ልጆች ኤላም፣ አሱር፣</td>\n",
              "      <td>የሴም ወንዶች ልጆች ኤላም፣ አሹር፣ አርፋክስድ፣ ሉድ እና አራም ነበሩ።</td>\n",
              "      <td>yasyeme wanidwoce lgoce 'elam, `ashure, `arifa...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>በንዳድ የሚያለቀው፣ ቤተስቦና በሌላ ሌላው እንደ ወባ ባለላ ሌላው በሽታ ...</td>\n",
              "      <td>በንዳድ የሚያልቀው፣ በተስቦና በሌላ ሌላው እንደ ወባ ባለው በሽታ የሚሰቃ...</td>\n",
              "      <td>bendadi yamiyaleqaw, betesibona' balela lyelaw...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>እስራኤላውያንን ከሰጥኋቸው ምድር ላይ አጠፋቸዋለሁ፤ እስራኤላውያንም በሕዝ...</td>\n",
              "      <td>እስራኤላውያንን ከሰጠኋቸው ምድር ላይ አጠፋቸዋለሁ፤ ለስሜ የቀደስኩትንም ...</td>\n",
              "      <td>isira'elawyanin kasatehuatchaw mdri laye 'atef...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>እኔም አየሁ፤ አንድ ንስር በሰማይ መከከል ድምፅ እንዲህ ሲል ሰማሁ፦</td>\n",
              "      <td>እኔም አየሁ፤ አንድ ንስር በሰማይ መካከል እየበረረ በታላቅ ድምፅ እንዲህ...</td>\n",
              "      <td>inyemi ajahu; `ande nsr besemaje makakale `eya...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>ባለድርሻ አካላት፤</td>\n",
              "      <td>ባለድርሻ አካላት፤</td>\n",
              "      <td>balederisa akalati;</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>የምርመራ ሹሙ ቢሮ ተጠርቼ ቀረብኩ።</td>\n",
              "      <td>የምርመራ ሹሙ ቢሮ ተጠርቼ ቀረብኩ።</td>\n",
              "      <td>jameremara xumu biro tateretche qerebiku.</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                result  ... is_correct\n",
              "0    ከዘቃዘ ውሃ የዛላችን ንፍስ እንደሚያረከ ሁሉክሩቅ አገር የመጣ መልካም ወ...  ...      False\n",
              "1                         በማልካም ሁኔታ የሚያስተማር ተገጣው ይገባል።  ...      False\n",
              "2        መጽሐፍ ቅዱስ በመጨረሻቹ ቀናቴ በካፍተኛ ሁኔታ እንደምበባስ ይነግራናል።  ...      False\n",
              "3                         የአክሱም ሐውልት ሁለተኛው ክፋይ አቅሱም ገባ  ...      False\n",
              "4                               ያሴም ወንዶች ልጆች ኤላም፣ አሱር፣  ...      False\n",
              "..                                                 ...  ...        ...\n",
              "495  በንዳድ የሚያለቀው፣ ቤተስቦና በሌላ ሌላው እንደ ወባ ባለላ ሌላው በሽታ ...  ...      False\n",
              "496  እስራኤላውያንን ከሰጥኋቸው ምድር ላይ አጠፋቸዋለሁ፤ እስራኤላውያንም በሕዝ...  ...      False\n",
              "497       እኔም አየሁ፤ አንድ ንስር በሰማይ መከከል ድምፅ እንዲህ ሲል ሰማሁ፦   ...      False\n",
              "498                                        ባለድርሻ አካላት፤  ...       True\n",
              "499                             የምርመራ ሹሙ ቢሮ ተጠርቼ ቀረብኩ።  ...       True\n",
              "\n",
              "[500 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uf0XpcnoGcKx",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}