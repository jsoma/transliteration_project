{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCUNNJHkgxJA"
   },
   "source": [
    "# Pull in the transliteration table (Ethiopic --> Latin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "txSI12K3gxJC"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'\n",
    "headers = {'User-Agent': user_agent}\n",
    "response = requests.get('https://scriptsource.org/cms/scripts/page.php?item_id=entry_detail&uid=vsytndbyev', headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "85Kp0V2ygxJK",
    "outputId": "c8801780-db7c-47b7-8e6d-d9674fa1ce50"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.read_html(response.text)\n",
    "df = results[1]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "8aO31pWNgxJS",
    "outputId": "951bbfb2-fd9c-423f-b68f-178334798bf1"
   },
   "outputs": [],
   "source": [
    "df.Glyph = df.USV.apply(lambda val: chr(int(val, 16)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rMrK8LbzgxJa"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"transliteration-table.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QkxcLwDagxJh"
   },
   "source": [
    "## Simplify Transliteration Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "colab_type": "code",
    "id": "xAxd99UbgxJj",
    "outputId": "2d8ceac3-9321-4d28-93a1-8407415cec81"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"transliteration-table.csv\")\n",
    "df = df.drop(columns=['USV'])\n",
    "df = df.set_index('Glyph')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "n4j13bd2gxJx",
    "outputId": "dc4112ce-96bf-4b6f-9287-8750b4614b04"
   },
   "outputs": [],
   "source": [
    "!pip install Unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "colab_type": "code",
    "id": "ouljoUG-gxJ3",
    "outputId": "e50f8ed4-7728-4729-de7d-d2026ba03400"
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import numpy as np\n",
    "\n",
    "def simplify(value):\n",
    "    try:\n",
    "        return unidecode.unidecode(value).lower()\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "simplified = df.applymap(simplify)\n",
    "merged = df.join(simplified, rsuffix='_simp')\n",
    "merged.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "oYbMDRzEgxKD",
    "outputId": "ad2029d0-e4c7-4a5b-f825-c413f7548e94"
   },
   "outputs": [],
   "source": [
    "uniqued = df.fillna('').apply(lambda row: list(set([v for v in row.values if '/' not in str(v)])), axis=1)\n",
    "uniqued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vTE2mdTKgxKI"
   },
   "outputs": [],
   "source": [
    "uniqued.to_json(\"lookup-table.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jcyLXxylgxKQ",
    "outputId": "4bde35d5-2b2e-4bb5-d7ab-ad9472a98f4e"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"lookup-table.json\") as f:\n",
    "    lookup = json.load(f)\n",
    "\n",
    "punctuation = [\n",
    "    ['።', '.'],\n",
    "    ['፡', ' '],\n",
    "    ['፣', ','],\n",
    "    ['፤', ';'],\n",
    "    ['፥', ':'],\n",
    "    ['፧', '?']\n",
    "]\n",
    "\n",
    "for punc in punctuation:\n",
    "    lookup[punc[0]] = [punc[1]]\n",
    "\n",
    "lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ChWmngiP7hak",
    "outputId": "bce126fd-4314-482d-b24b-50b165ce189c"
   },
   "outputs": [],
   "source": [
    "lookup_simp = {}\n",
    "for key in lookup.keys():\n",
    "    values = list(set([unidecode.unidecode(v).lower() for v in lookup[key]]))\n",
    "    lookup_simp[key] = [v for v in values if '@' not in v]\n",
    "    \n",
    "lookup_simp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3sbaKBNSNNn"
   },
   "source": [
    "# Create reverse dictionary (Latin --> Ethiopic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xWKtIPlWgxKf",
    "outputId": "9d0dae06-b6fa-41ac-9c08-f1318fe524e6"
   },
   "outputs": [],
   "source": [
    "# create reverse dictionary\n",
    "lat_eth = dict()\n",
    "\n",
    "for key in lookup_simp.keys():\n",
    "    values = lookup_simp[key]\n",
    "    for v in values:\n",
    "        if v in lat_eth.keys():\n",
    "            new_list = lat_eth[v].copy()\n",
    "            new_list.append(key)\n",
    "            lat_eth[v] = new_list\n",
    "        else:\n",
    "            lat_eth[v] = [key]\n",
    "\n",
    "lat_eth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OF2nht_ASUV3"
   },
   "source": [
    "# Transliteration work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n4AgAJlpiUfo"
   },
   "outputs": [],
   "source": [
    "# load dictionary to prune transliteration options \n",
    "am_dic_file = open(\"am_dic.txt\", \"r\", encoding=\"utf-8\")\n",
    "am_dic = []\n",
    "\n",
    "for w in am_dic_file.readlines():\n",
    "    am_dic.append(w.rstrip())\n",
    "\n",
    "am_dic_file.close()\n",
    "am_dic = set(am_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ngOAyW6SXw1"
   },
   "source": [
    "## Use CountVectorizer() to do char n grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "wVejetZNSgDR",
    "outputId": "840899da-3003-4918-f213-31ca669fa9d3"
   },
   "outputs": [],
   "source": [
    "# clean corpus\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "file = open(\"raw/new-am.txt\", \"r\", encoding=\"utf-8\")\n",
    "corpus = file.read()\n",
    "\n",
    "# tokenize corpus (https://machinelearningmastery.com/clean-text-machine-learning-python/)\n",
    "tokens = word_tokenize(corpus)\n",
    "# remove all tokens that are not alphabetic\n",
    "tokens = [word for word in tokens if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T9nWKo1cSl-b"
   },
   "outputs": [],
   "source": [
    "# train model to do ngram work\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(1, 3), analyzer=\"char_wb\")\n",
    "cv_fit = cv.fit_transform(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M6brLQj2S0Sx",
    "outputId": "19e4228c-53ed-47bf-c773-fd4f88ebb2d9"
   },
   "outputs": [],
   "source": [
    "print(\"Vocabulary size 1-3gram: \", len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "XWBFPHlKS7jh",
    "outputId": "c6241cf1-982d-463b-ee66-3c66d1fcf895"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ngram_list = cv.get_feature_names()\n",
    "count_list = np.asarray(cv_fit.sum(axis=0))[0]\n",
    "\n",
    "# make a dictionary with frequencies \n",
    "freq_dict = dict(zip(ngram_list,count_list))\n",
    "\n",
    "# get unigram, bigram, trigram total counts\n",
    "unigram_count = 0\n",
    "bigram_count = 0\n",
    "trigram_count = 0\n",
    "\n",
    "for key in freq_dict.keys():\n",
    "    if len(key)==1:\n",
    "        unigram_count += freq_dict[key]\n",
    "    elif len(key)==2:\n",
    "        bigram_count += freq_dict[key]\n",
    "    else:\n",
    "        trigram_count += freq_dict[key]\n",
    "\n",
    "print(\"Unigram Count: \", unigram_count)\n",
    "print(\"Bigram Count: \", bigram_count)\n",
    "print(\"Trigram Count: \", trigram_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XYbXyZwgXcSh"
   },
   "source": [
    "## Actual Transliterating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('lat_eth.json', 'w') as f:\n",
    "    json.dump(lat_eth, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breakdowns(segment):\n",
    "    options = []\n",
    "    max_len = min([len(segment), 4])\n",
    "    for i in range(1, max_len+1):\n",
    "        potential = segment[:i]\n",
    "        if potential in lat_eth.keys():\n",
    "            remainder = segment[i:]\n",
    "            if remainder == \"\":\n",
    "                options.append([potential])\n",
    "            else:\n",
    "                enders = get_breakdowns(remainder)\n",
    "                if enders == []:\n",
    "                    return []\n",
    "                else:\n",
    "                    options.extend([potential, *e] for e in enders)\n",
    "    return options\n",
    "    \n",
    "\n",
    "breakdowns = get_breakdowns(\"inidihi\")\n",
    "breakdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def convert(segmentation):\n",
    "    final_list = []\n",
    "    relevant_lists = []\n",
    "    for elmt in segmentation:\n",
    "        relevant_lists.append(lat_eth[elmt])\n",
    "    for i in itertools.product(*relevant_lists):\n",
    "        final_list.append(''.join(i))\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eth_options = convert(['i', 'n', 'i', 'd', 'i', 'h', 'i'])\n",
    "eth_options[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"char2idx.json\") as f:\n",
    "    char2idx = json.load(f)\n",
    "\n",
    "with open(\"idx2char.json\") as f:\n",
    "    idx2char = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import keras\n",
    "\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model = tf.keras.models.load_model(\"char_model\", compile=False)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_string = \"በ\"\n",
    "temperature = 1.0\n",
    "num_generate = 1\n",
    "input_eval = [char2idx[s] for s in start_string]\n",
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "model.reset_states()\n",
    "predictions = model(input_eval)\n",
    "predictions = tf.squeeze(predictions, 0)\n",
    "predictions = predictions / temperature\n",
    "\n",
    "predicted_id = np.argsort(predictions)[0, -3:]\n",
    "predicted_id\n",
    "print(\"\".join(idx2char[predicted_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5tlM_qWNgxKl"
   },
   "outputs": [],
   "source": [
    "# generate possible transliterations\n",
    "\n",
    "import string\n",
    "import itertools\n",
    "import operator\n",
    "import string \n",
    "import random\n",
    "\n",
    "# given a sentence in latin characters, splits and sends word by word to the \n",
    "# function transliterate_word\n",
    "def transliterate(text):\n",
    "    sent_trans = []\n",
    "\n",
    "    sentence = [i for j in text.split() for i in (j, ' ')][:-1]\n",
    "    cleaned = []\n",
    "    for elmt in sentence:\n",
    "      elmt_tokenized = word_tokenize(elmt)\n",
    "      if elmt == ' ':\n",
    "        cleaned.append(' ')\n",
    "      elif len(elmt) == len(elmt_tokenized):\n",
    "        cleaned.append(elmt)\n",
    "      else:\n",
    "        for i in elmt_tokenized:\n",
    "          if i == \"'\":\n",
    "            elmt_tokenized.remove(i)\n",
    "        cleaned += elmt_tokenized\n",
    "\n",
    "    for word in cleaned:\n",
    "      sent_trans.append(transliterate_word(word))\n",
    "    \n",
    "    return \"\".join(sent_trans)\n",
    "\n",
    "\n",
    "# transliterate_word returns spaces/punctuations as appropriate\n",
    "# and sends an actual latin character word to ngram_selected(word) to \n",
    "# obtain the appropriate transliterated word in ethiopic\n",
    "def transliterate_word(word):\n",
    "    # print(\"transliterating the word\")\n",
    "    if word in string.punctuation and word not in lat_eth.keys():\n",
    "      return word\n",
    "    elif word.isnumeric() == True:\n",
    "      return word\n",
    "    elif word == \" \":\n",
    "      return word\n",
    "    elif len(word) > 15:\n",
    "      return word\n",
    "    else:\n",
    "      word = unidecode.unidecode(word).lower()\n",
    "      # print(f\"Sending {word} to ngram_selected\")\n",
    "      return ngram_selected(word)\n",
    "\n",
    "# ngram_selected takes a latin character word and generates all possible ethiopic\n",
    "# transliterations by calling the function possibilities; it then selects the \n",
    "# ethiopic option with the highest score using the function word_score\n",
    "def ngram_selected(word):\n",
    "    options = possibilities(word)\n",
    "    # print('possibilities are', options)\n",
    "    if len(options) == 0:\n",
    "      return word\n",
    "    else:\n",
    "      scores = dict()\n",
    "      for opt in options:\n",
    "        score = word_score(opt)\n",
    "        scores[opt] = score\n",
    "      # print('scores were calculated as', scores)\n",
    "      selected_word = max(scores.items(), key=operator.itemgetter(1))[0]\n",
    "      return selected_word\n",
    "\n",
    "# the function possibilities takes a latin character word and returns all \n",
    "# possible transliterations into ethiopic based on the reverse dictionary\n",
    "# this function calls the function prune to remove entries that are \n",
    "# not in an actual amharic dictionary (unless pruning results in 0 options)   \n",
    "# this function also calls the function convert to go from latin char to \n",
    "# ethiopic char as based on the reverse dictionary\n",
    "def possibilities(word):\n",
    "#     print(\"calculating possibilities\")\n",
    "    # split word into chars\n",
    "    chars = list(word)\n",
    "    \n",
    "    # generate all combinations \n",
    "    # https://stackoverflow.com/questions/27263155/python-find-all-possible-\n",
    "    # word-combinations-with-a-sequence-of-characters-word\n",
    "#     print(\"all combinations of\", chars)\n",
    "    combinatorics = itertools.product([True, False], repeat=len(chars) - 1)\n",
    "    latin_segmentation = []\n",
    "    add = True\n",
    "    for combination in combinatorics:\n",
    "#         print(\"looking at\", combination)\n",
    "        i = 0\n",
    "        one_such_combination = [chars[i]]\n",
    "        for slab in combination:\n",
    "            # print(\"slab is\", slab)\n",
    "            i += 1\n",
    "            if not slab: # there is a join\n",
    "                one_such_combination[-1] += chars[i]\n",
    "            else:\n",
    "                one_such_combination += [chars[i]]\n",
    "        \n",
    "        for elmt in one_such_combination:\n",
    "#             print(\"Looking up\", elmt)\n",
    "            if elmt not in lat_eth.keys():\n",
    "                add = False\n",
    "                break\n",
    "        # only add/consider if segmentation can be converted into ethiopic \n",
    "        # characters\n",
    "        if add == True:  \n",
    "            latin_segmentation.append(one_such_combination)\n",
    "            \n",
    "        # reset\n",
    "        add = True\n",
    "    \n",
    "    # conversion\n",
    "    ethiopic_opts = []\n",
    "    for segmentation in latin_segmentation:\n",
    "        ethiopic_opts += convert(segmentation)\n",
    "\n",
    "    pruned = prune(ethiopic_opts)\n",
    "    if len(pruned) == 0:\n",
    "      if len(ethiopic_opts) < 100:\n",
    "        return ethiopic_opts\n",
    "      else:\n",
    "        sampling = random.choices(ethiopic_opts, k=99)\n",
    "        return sampling\n",
    "    else:\n",
    "      return pruned\n",
    "\n",
    "# this is called by the function possibilities to convert from latin char\n",
    "# to ethiopic char given a particular segmentation (i.e. i-di vs. i-d-i might\n",
    "# both be sent separately)\n",
    "def convert(segmentation):\n",
    "    final_list = []\n",
    "    relevant_lists = []\n",
    "    for elmt in segmentation:\n",
    "        relevant_lists.append(lat_eth[elmt])\n",
    "    for i in itertools.product(*relevant_lists):\n",
    "        final_list.append(''.join(i))\n",
    "    return final_list\n",
    "\n",
    "# this is called by the function possibilities to prune the list of possible\n",
    "# ethiopic transliterations\n",
    "def prune(possibilities):\n",
    "    final_possibilities = []\n",
    "    for candidate in possibilities: \n",
    "      if candidate in am_dic:\n",
    "          final_possibilities.append(candidate)\n",
    "    return final_possibilities\n",
    "\n",
    "# this function is called by ngram_selected to determine the probability of\n",
    "# an ethiopic word occurring (using ngram counts)\n",
    "# this function calls get_ngrams to split the given word into n-length \n",
    "# subsections for scoring\n",
    "# this function also calls one or multiple of the [n]gram_probability functions \n",
    "# to compute each [n]gram score, which are then weighted evenly in computing \n",
    "# the final score\n",
    "def word_score(word):\n",
    "  sequence = list(word)\n",
    "  if len(word) >= 3:\n",
    "    # calculate trigram probability\n",
    "    w = 1/3.0\n",
    "    trigrams = get_ngrams(sequence, 3)\n",
    "    bigrams = get_ngrams(sequence, 2)\n",
    "    unigrams = get_ngrams(sequence, 1)\n",
    "    score_t = trigram_probability(trigrams)\n",
    "    score_b = bigram_probability(bigrams)\n",
    "    score_u = unigram_probability(unigrams)\n",
    "    score = (w*score_t)+(w*score_b)+(w*score_u) \n",
    "  elif len(word) >= 2:\n",
    "    # calculate bigram probability\n",
    "    w = 1/2.0\n",
    "    bigrams = get_ngrams(sequence, 2)\n",
    "    unigrams = get_ngrams(sequence, 1)\n",
    "    score_b = bigram_probability(bigrams)\n",
    "    score_u = unigram_probability(unigrams)\n",
    "    score = (w*score_b)+(w*score_u) \n",
    "  else:\n",
    "    # calculate unigram probability\n",
    "    unigrams = get_ngrams(sequence, 1)\n",
    "    score = unigram_probability(word)\n",
    "  return score\n",
    "\n",
    "# called by the function word_score to generate n gram subsections \n",
    "# from a given ethiopic word\n",
    "def get_ngrams(sequence, n):\n",
    "    input = sequence\n",
    "    output = []\n",
    "    for i in range(0, len(input) - n + 1):\n",
    "        output.append(input[i:i + n])\n",
    "    \n",
    "    return [''.join(l) for l in output]\n",
    "\n",
    "# these funtions are called by word_score to compute [n]gram probabilities given\n",
    "# an ethiopic word \n",
    "def trigram_probability(trigrams):\n",
    "  freq = 0\n",
    "  for t in trigrams:\n",
    "    # get freq\n",
    "    if t in freq_dict:\n",
    "      freq += freq_dict[t]\n",
    "  avg_prob = freq/(len(trigrams) * trigram_count)\n",
    "  return avg_prob\n",
    "\n",
    "def bigram_probability(bigrams):\n",
    "  freq = 0\n",
    "  for b in bigrams:\n",
    "    # get freq\n",
    "    if b in freq_dict:\n",
    "      freq += freq_dict[b]\n",
    "  avg_prob = freq/(len(bigrams) * bigram_count)\n",
    "  return avg_prob\n",
    "\n",
    "def unigram_probability(unigrams):\n",
    "  freq = 0\n",
    "  for u in unigrams:\n",
    "    # get freq\n",
    "    if u in freq_dict:\n",
    "      freq += freq_dict[u]\n",
    "  avg_prob = freq/(len(unigrams) * unigram_count)\n",
    "  return avg_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'እንዲህ'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_selected(\"inidihi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gVLCO1_WYkuA",
    "outputId": "c4c0cf2c-c244-44b1-92ec-58875f8ebbac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'እንዲህ ስል ። ሁለት ሰዎች ሊጸልዩ ወደ'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transliterate(\"inidihi sil . hulat sewochi liseliyu wade\")\n",
    "#እንዲህ ሲል ። ሁለት ሰዎች ሊጸልዩ ወደ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xc_m4oWrZD9q",
    "outputId": "3eb6c172-b568-437a-c30c-127b6ac32858"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ኢየሱስም ። እውንት እውነት እላችኋለሁ'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transliterate(\"ijasusme . 'eweneti iwnat `elacehualehu\")\n",
    "#ኢየሱስም ። እውነት እውነት እላችኋለሁ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OKLZwtMwZwJy",
    "outputId": "27c328f4-9e06-4916-d587-c687bbb824ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'አባቱን ወይም እናቱን አያከብእርም ትላላችሁ ፤'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transliterate(\"abatune wejeme ina'tun ajakebirm tlalacihu ;\")\n",
    "#አባቱን ወይም እናቱን አያከብርም ትላላችሁ ፤ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Mn9z8eBgZ9gQ",
    "outputId": "519000fa-f29f-4493-d7a5-db2ebbc6d13a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'በምን አይነትም ሞት ይሞት ዘንድ እንዳለው'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transliterate(\"bamine 'ajenetim mote ymoti zendi `enidalawe\")\n",
    "#በምን ዓይነትም ሞት ይሞት ዘንድ እንዳለው"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9xIKPBjwBx3R"
   },
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-Exg0aSnYR1H",
    "outputId": "c5bdb007-79ec-4523-f71a-cb4435e4cbc9"
   },
   "outputs": [],
   "source": [
    "! pip install python-levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b32ewnOKWAPc"
   },
   "outputs": [],
   "source": [
    "# EVALUATION\n",
    "\n",
    "import Levenshtein\n",
    "from statistics import mean\n",
    "\n",
    "# this function takes two files (parallel texts) and generates\n",
    "# two lists after stripping beginning/trailing whitespace\n",
    "def extract_from_files(ethiopic_file, latin_file):\n",
    "  eth_file = open(ethiopic_file)\n",
    "  lat_file = open(latin_file)\n",
    "  ethiopic = [line.rstrip() for line in eth_file.readlines()]\n",
    "  latin = [line.rstrip() for line in lat_file.readlines()] \n",
    "  \n",
    "  ethiopic = list(ethiopic)\n",
    "  latin = list(latin)\n",
    "\n",
    "  return ethiopic, latin\n",
    "\n",
    "# this function takes two parallel lists and evaluates how\n",
    "# our model performs\n",
    "def evaluate(ethiopic, latin):\n",
    "  accuracies = []\n",
    "\n",
    "  count = 0\n",
    "  for line in latin:\n",
    "    predict = transliterate(line)\n",
    "    correct = ethiopic[count]\n",
    "    accuracy = Levenshtein.ratio(predict,correct)\n",
    "    accuracies.append(accuracy) \n",
    "    count += 1\n",
    "\n",
    "  return mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def evaluate_top_n(ethiopic, latin, top_n=2):\n",
    "    accuracies = []\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for eth_line, lat_line in zip(ethiopic, latin):\n",
    "        eth_tokens = re.split('\\s+', eth_line.strip())\n",
    "        lat_tokens = re.split('\\s+', lat_line.strip())\n",
    "        for eth_token, lat_token in zip(eth_tokens, lat_tokens):\n",
    "            results = transliterate_word(lat_token, top_n=3)\n",
    "            print(eth_token, results, lat_token)\n",
    "            if eth_token in results:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    return correct/total\n",
    "\n",
    "def transliterate_word(word, top_n=1):\n",
    "    if word.isspace():\n",
    "        return word\n",
    "    if word in lat_eth.keys() and len(lat_eth[word]) == 0:\n",
    "        return lat_eth[word][0]\n",
    "    elif word in string.punctuation:\n",
    "        return word\n",
    "    elif word.isnumeric() == True:\n",
    "        return word\n",
    "    else:\n",
    "        result = ngram_selected(word)\n",
    "        return [result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uI7hF_ASVpod"
   },
   "source": [
    "## first on google translated pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CaB6cgFgB0m3"
   },
   "outputs": [],
   "source": [
    "ethiopic_tot, latin_tot = extract_from_files(\"raw/original.txt\", \"raw/transliterated.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcggmxtcbDMk"
   },
   "outputs": [],
   "source": [
    "ethiopic_2 = ethiopic_tot[0:1]\n",
    "latin_2 = latin_tot[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xNN6j0IBbKQ_",
    "outputId": "b185e0aa-b643-45e1-dbb2-431c42e659ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“የዳታ ['የዳታ'] jadata\n",
      "ፕሮሰሲንግ ['ጵርውኦጸስንኤግእ'] perwosasinegi\n",
      "አገልግሎት” ['አገልግሎት'] `ageligelwote\n",
      "ማለት ['ማለት'] maleti\n",
      "በኮምፒዩተር ['በቾምኤፕእይዑጥአርእ'] becomepiyutari\n",
      "ሥርዓት ['ስርአት'] siri`at\n",
      "አማካኝነት ['አማካኝነት'] `amakannate\n",
      "ዳታን ['ዳታን'] datan\n",
      "የመቀበል ['የምችዓበል'] yemecabal\n",
      "፣ , ,\n",
      "የማከማቸት ['ያምዓችማከት'] jamacemaket\n",
      "፣ , ,\n",
      "የመተንተን ['ጅአመትአንትኧን'] jamatanitan\n",
      "፣ , ,\n",
      "የማሰራጨት ['የመሰረት'] yemasarat\n",
      "፣ , ,\n",
      "የማጓጓዝ ['የምአግወግውአዝ'] yamagwagwaz\n",
      "ወይም ['ወይም'] wayim\n",
      "የማስተላለፍ ['የማስተላለፍ'] yemastelalafi\n",
      "አገልግሎት ['አገልግሎት'] 'agelglwoti\n",
      "ሲሆን ['ስሆን'] sihwone\n",
      "የኔትዎርክ ['የንተዖርኢክ'] yeneteworik\n",
      "አገልግሎችንም ['አጘልገልኦንመ'] agaligelonme\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2608695652173913"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_top_n(ethiopic_2, latin_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zq_HhehoblYF"
   },
   "outputs": [],
   "source": [
    "ethiopic_3 = ethiopic_tot[0:100]\n",
    "latin_3 = latin_tot[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6TqMTQoobriB",
    "outputId": "6d8c6fe6-1d80-4093-fe8c-1f0a8caf6b3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 47s, sys: 32.5 s, total: 5min 19s\n",
      "Wall time: 5min 52s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7330918000222264"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(ethiopic_3, latin_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3f-Gw_wiCO7I"
   },
   "outputs": [],
   "source": [
    "ethiopic = ethiopic_tot[0:1000]\n",
    "latin = latin_tot[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SwGxMpJqEgf2",
    "outputId": "1bf976f3-cada-4668-a2e4-0fd75f98c079"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-a356d7e040f9>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(ethiopic, latin)\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlatin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransliterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0methiopic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLevenshtein\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-e589d7d04ff6>\u001b[0m in \u001b[0;36mtransliterate\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcleaned\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m       \u001b[0msent_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransliterate_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_trans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-e589d7d04ff6>\u001b[0m in \u001b[0;36mtransliterate_word\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munidecode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munidecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m       \u001b[0;31m# print(f\"Sending {word} to ngram_selected\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mngram_selected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# ngram_selected takes a latin character word and generates all possible ethiopic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-e589d7d04ff6>\u001b[0m in \u001b[0;36mngram_selected\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# ethiopic option with the highest score using the function word_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mngram_selected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpossibilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;31m# print('possibilities are', options)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-e589d7d04ff6>\u001b[0m in \u001b[0;36mpossibilities\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0methiopic_opts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msegmentation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlatin_segmentation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0methiopic_opts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegmentation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mpruned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0methiopic_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-e589d7d04ff6>\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(segmentation)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mrelevant_lists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlat_eth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melmt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrelevant_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mfinal_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(ethiopic,latin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FWvnuQ1cgRjN",
    "outputId": "4bf22f67-e8a1-4c16-f58b-5b66fa219d59"
   },
   "outputs": [],
   "source": [
    "num_words = 0\n",
    "\n",
    "for line in ethiopic_tot[0:1000]:\n",
    "  num_words += len(line)\n",
    "num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mtBrMuWyVtiN"
   },
   "source": [
    "## now on manually generated parallel texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4e6uVXTAN-7M"
   },
   "outputs": [],
   "source": [
    "ethiopic_manual, latin_manual = extract_from_files('taitu_am.txt','taitu_rom.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NFmxBojOPa6B",
    "outputId": "62164aef-208d-48b4-9be1-f254dadd5fba"
   },
   "outputs": [],
   "source": [
    "evaluate(ethiopic_manual, latin_manual)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "pk_work_4/29_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
