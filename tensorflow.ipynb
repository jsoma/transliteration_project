{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ORy8sSAawbp-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f_CqDe3Z1VO_"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('lat_eth.json') as f:\n",
    "    lat_eth = json.load(f)\n",
    "\n",
    "eth2lat = {}\n",
    "for lat in lat_eth.keys():\n",
    "    for eth in lat_eth[lat]:\n",
    "        if eth not in eth2lat.keys():\n",
    "            eth2lat[eth] = []\n",
    "        eth2lat[eth].append(lat)\n",
    "\n",
    "for key in eth2lat.keys():\n",
    "    eth2lat[key] = set(eth2lat[key])\n",
    "\n",
    "with open('char2idx.json') as f:\n",
    "    char2idx = json.load(f)\n",
    "\n",
    "with open('idx2char.json') as f:\n",
    "    idx2char = np.array(json.load(f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "6UNFupcc6LQn",
    "outputId": "62723844-b11d-4751-f2e0-7fdae3026d07"
   },
   "outputs": [],
   "source": [
    "# Used a custom loss function so we need to load_model without compilation\n",
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model = tf.keras.models.load_model(\"char_model\", compile=False)\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability calculations\n",
    "\n",
    "Softmax converts the predictions into probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00088863523"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert predictions to probabilities\n",
    "\n",
    "cont_cached = {}\n",
    "\n",
    "def get_next_char_probs(start_string):\n",
    "    cache_key = start_string\n",
    "    if cache_key in cont_cached.keys():\n",
    "        pred_prob = cont_cached[cache_key]\n",
    "    else:\n",
    "        input_eval = [char2idx[s] for s in start_string]\n",
    "        input_eval = tf.expand_dims(input_eval, 0)\n",
    "        print(\"input_eval is\", input_eval)\n",
    "        # Ask model to evaluate what's next, given start_string\n",
    "        model.reset_states()\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        # Convert scores to probabilities\n",
    "        pred_prob = tf.nn.softmax(predictions[0])\n",
    "\n",
    "        # Cache probabilities\n",
    "        cont_cached[cache_key] = pred_prob\n",
    "    return pred_prob\n",
    "\n",
    "def continuation_proba(start_string, next_char):\n",
    "    try:\n",
    "        # Index of target for when we pull it out of the scores\n",
    "        target_idx = char2idx[next_char]\n",
    "    except:\n",
    "        # If we've never seen that char before, get out of here!\n",
    "        return 0\n",
    "    \n",
    "    # Check the cache\n",
    "    cache_key = start_string\n",
    "    if cache_key in cont_cached.keys():\n",
    "        pred_prob = cont_cached[cache_key]\n",
    "    else:\n",
    "        # Convert characters into indices for tensorflow processing\n",
    "        try:\n",
    "            input_eval = [char2idx[s] for s in start_string]\n",
    "        except:\n",
    "            # We've never seen it!!! just exit!!\n",
    "            return 0\n",
    "\n",
    "        input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "        # Ask model to evaluate what's next, given start_string\n",
    "        model.reset_states()\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        # Convert scores to probabilities\n",
    "        pred_prob = tf.nn.softmax(predictions[0])\n",
    "\n",
    "        # Cache probabilities\n",
    "        cont_cached[cache_key] = pred_prob\n",
    "\n",
    "    \n",
    "    proba = pred_prob[target_idx].numpy()\n",
    "    return proba\n",
    "\n",
    "start_string = \"ኢንኢድኢህኢ\"\n",
    "next_char = \"ን\"\n",
    "\n",
    "continuation_proba(start_string, next_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability of each step multiplied out\n",
    "\n",
    "Currently does not adjust for length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted for length:\n",
      "0.01510776486247778\n",
      "0.0004449678877608077\n",
      "2.7231123378621187e-05\n",
      "\n",
      "Not adjusted for length:\n",
      "0.00755388243123889\n",
      "0.00014832262925360256\n",
      "9.077041126207062e-06\n",
      "CPU times: user 38.4 ms, sys: 3.35 ms, total: 41.7 ms\n",
      "Wall time: 45.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cached = {}\n",
    "\n",
    "def text_proba(text, length_adj=False):\n",
    "    # Check the cache\n",
    "    cache_key = text + (\"-adj\" if length_adj else \"-unadj\")\n",
    "    if cache_key in cached.keys():\n",
    "        return cached[cache_key]\n",
    "\n",
    "    # Otherwise we'll calculate the probability\n",
    "    proba = 1\n",
    "    for i in range(1, len(text)):\n",
    "        base_str = text[:i]\n",
    "        next_char = text[i]\n",
    "        proba = proba * continuation_proba(base_str, next_char)\n",
    "\n",
    "    # Adjust for length\n",
    "    if length_adj:\n",
    "        proba = len(text) * proba\n",
    "\n",
    "    # Store in cache\n",
    "    cached[cache_key] = proba\n",
    "    return proba\n",
    "\n",
    "print(\"Adjusted for length:\")\n",
    "print(text_proba(\"ጭም\", length_adj=True))\n",
    "print(text_proba(\"ጭምቅ\", length_adj=True))\n",
    "print(text_proba(\"ጭምኢ\", length_adj=True))\n",
    "\n",
    "print(\"\\nNot adjusted for length:\")\n",
    "print(text_proba(\"ጭም\", length_adj=False))\n",
    "print(text_proba(\"ጭምቅ\", length_adj=False))\n",
    "print(text_proba(\"ጭምኢ\", length_adj=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# REMOVED because the greedy one (below) works better\n",
    "# but left here for posterity\n",
    "#\n",
    "#\n",
    "\n",
    "# %%time\n",
    "# import itertools\n",
    "# MAX_SEQ_LENGTH = 5\n",
    "\n",
    "# def predict(sequence, base=None):\n",
    "#     # print(\"Base of\", base)\n",
    "#     if len(sequence) > MAX_SEQ_LENGTH:\n",
    "#         starters = predict(sequence[:MAX_SEQ_LENGTH], base)\n",
    "#         overall_results = []\n",
    "#         for r in starters:\n",
    "#             if base:\n",
    "#                 text = base + r[0]\n",
    "#             else:\n",
    "#                 text = r[0]\n",
    "#             results = predict(sequence[MAX_SEQ_LENGTH:], text)\n",
    "#             overall_results.extend(results)\n",
    "#         return sorted(overall_results,\n",
    "#                       key=lambda result: result[1],\n",
    "#                       reverse=True)[:5]\n",
    "#     else:\n",
    "#         expanded = itertools.product(*sequence[:MAX_SEQ_LENGTH])\n",
    "#         amharic_options = [''.join(chars) for chars in expanded]\n",
    "#         if base:\n",
    "#             amharic_options = [base + text for text in amharic_options]\n",
    "#         scores = []\n",
    "#         for possible in amharic_options:\n",
    "#             scores.append(text_proba(''.join(possible)))\n",
    "        \n",
    "#         top_5 = np.argsort(scores)[-5:][::-1]\n",
    "#         amharic_options = np.array(amharic_options)\n",
    "#         scores = np.array(scores)\n",
    "        \n",
    "#         return list(zip(amharic_options[top_5], scores[top_5]))\n",
    "\n",
    "# def breakdown_and_predict(text):\n",
    "#     top_fives = []\n",
    "#     for segmentation in tqdm_notebook(get_breakdowns(text), position=0):\n",
    "#         options = [lat_eth[segment] for segment in segmentation]\n",
    "#         top_five = predict(options)\n",
    "#         top_fives.extend(top_five)\n",
    "\n",
    "#     # We'll return the top 10 top fives\n",
    "#     return sorted(top_fives,\n",
    "#               key=lambda result: result[1],\n",
    "#               reverse=True)[:10]\n",
    "\n",
    "# # ጥበብን\n",
    "# breakdown_and_predict(\"tibebini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breakdowns(segment):\n",
    "    options = []\n",
    "    max_len = min([len(segment), 4])\n",
    "    for i in range(1, max_len+1):\n",
    "        potential = segment[:i]\n",
    "        if potential in lat_eth.keys():\n",
    "            remainder = segment[i:]\n",
    "            if remainder == \"\":\n",
    "                options.append([potential])\n",
    "            else:\n",
    "                enders = get_breakdowns(remainder)\n",
    "                if enders == []:\n",
    "                    return []\n",
    "                else:\n",
    "                    options.extend([potential, *e] for e in enders)\n",
    "    return options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'n', 'i', 'd', 'i', 'h', 'i'],\n",
       " ['i', 'n', 'i', 'd', 'i', 'hi'],\n",
       " ['i', 'n', 'i', 'di', 'h', 'i'],\n",
       " ['i', 'n', 'i', 'di', 'hi'],\n",
       " ['i', 'ni', 'd', 'i', 'h', 'i'],\n",
       " ['i', 'ni', 'd', 'i', 'hi'],\n",
       " ['i', 'ni', 'di', 'h', 'i'],\n",
       " ['i', 'ni', 'di', 'hi']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breakdowns = get_breakdowns(\"inidihi\")\n",
    "breakdowns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-greedy algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that text_proba does NOT work with single chars, so this might be off at some points?\n",
    "\n",
    "## `best_next_steps`\n",
    "\n",
    "It's like \"given a string, what's our next best character option?\", except we allow for multiple inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of 'current' states (e.g. top 3 'tibe' options)\n",
    "# And a list of things to add on (e.g. all 'bi' options)\n",
    "# What are the best n options for 'tibebi'\n",
    "def best_next_steps(current_states, next_options, n=3):\n",
    "    # Pairs keeps track of all texts + probabilities\n",
    "    pairs = []\n",
    "    \n",
    "    # From each possible \n",
    "    for base in current_states:\n",
    "        # Create all the text options we're looking at\n",
    "        # Then calculate their probability\n",
    "        texts = [f\"{base}{follower}\" for follower in next_options]\n",
    "        probs = [text_proba(text) for text in texts]\n",
    "        pairs.extend(zip(texts, probs))\n",
    "    \n",
    "    # Only return the top n options\n",
    "    top = sorted(pairs, key=lambda pair: pair[1], reverse=True)[:n]\n",
    "\n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `get_top_sequences` and `get_top_sequences_all`\n",
    "\n",
    "Given a breakdown (or a list of breakdowns), what are the best Ethiopic options? Greedy-ish, only follows the top 3 by default but you can adjust it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Receive a list like ['ti', 'be', 'bi', 'ni']\n",
    "# Tries out each Ethiopic option, returns top n most likely\n",
    "def get_top_sequences(breakdown, n=3, lower_bound=None):\n",
    "    eth_poss = [lat_eth[latin] for latin in breakdown]\n",
    "\n",
    "    current = eth_poss[0]\n",
    "    for next_options in eth_poss[1:]:\n",
    "        top = best_next_steps(current, next_options, n)\n",
    "\n",
    "        # Only keep ones above lower score bound\n",
    "        # if you want to use them all, lower_bound should be None\n",
    "        if lower_bound != None:\n",
    "            current = [option[0] for option in top if option[1] > lower_bound]\n",
    "            # For debugging\n",
    "            dropped = len(top) - len(current)\n",
    "        else:\n",
    "            current = [option[0] for option in top]\n",
    "    return top\n",
    "\n",
    "# Takes a list of potential breakdowns\n",
    "# n is passed to get_top_sequences\n",
    "# limit is used here\n",
    "def get_top_sequences_all(breakdowns, limit=5, n=3):\n",
    "    tops = []\n",
    "    \n",
    "    # shorter breakdowns go first\n",
    "    breakdowns = sorted(breakdowns, key=lambda breakdown: len(breakdowns))\n",
    "\n",
    "    lower_bound = 0\n",
    "    for breakdown in tqdm_notebook(breakdowns):\n",
    "        top = get_top_sequences(breakdown, n, lower_bound=lower_bound)\n",
    "        if len(top) > 0:\n",
    "            worst_score = top[-1][1]\n",
    "            if worst_score > lower_bound:\n",
    "                lower_bound = worst_score\n",
    "            tops.extend(top)\n",
    "        \n",
    "    return sorted(tops, key=lambda pair: pair[1], reverse=True)[:limit]\n",
    "\n",
    "# Given Latin script, what is the best Ethiopic?\n",
    "def top_transcriptions(text, limit=5, n=3):\n",
    "    breakdowns = get_breakdowns(text)\n",
    "    return get_top_sequences_all(breakdowns, limit, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['t', 'i', 'b', 'e', 'b', 'i', 'n', 'i'],\n",
       " ['t', 'i', 'b', 'e', 'b', 'i', 'ni'],\n",
       " ['t', 'i', 'b', 'e', 'bi', 'n', 'i'],\n",
       " ['t', 'i', 'b', 'e', 'bi', 'ni'],\n",
       " ['t', 'i', 'be', 'b', 'i', 'n', 'i'],\n",
       " ['t', 'i', 'be', 'b', 'i', 'ni'],\n",
       " ['t', 'i', 'be', 'bi', 'n', 'i'],\n",
       " ['t', 'i', 'be', 'bi', 'ni'],\n",
       " ['ti', 'b', 'e', 'b', 'i', 'n', 'i'],\n",
       " ['ti', 'b', 'e', 'b', 'i', 'ni'],\n",
       " ['ti', 'b', 'e', 'bi', 'n', 'i'],\n",
       " ['ti', 'b', 'e', 'bi', 'ni'],\n",
       " ['ti', 'be', 'b', 'i', 'n', 'i'],\n",
       " ['ti', 'be', 'b', 'i', 'ni'],\n",
       " ['ti', 'be', 'bi', 'n', 'i'],\n",
       " ['ti', 'be', 'bi', 'ni']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting potential breakdowns\n",
    "get_breakdowns('tibebini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ኢ', 'እ', 'ዒ', 'ዕ'],\n",
       " ['ኒ', 'ን', 'ኚ', 'ኝ'],\n",
       " ['ዲ', 'ድ', 'ዺ', 'ፂ'],\n",
       " ['ሂ', 'ህ', 'ሒ', 'ሕ', 'ኂ', 'ኅ', 'ኺ', 'ኽ', 'ⷒ']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting from latin to potential ethiopic\n",
    "# (only works for one breakdown at a time)\n",
    "breakdown = breakdowns[-1]\n",
    "[lat_eth[segment] for segment in breakdown]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001135805060129536"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate text probability\n",
    "text_proba('ጥበብ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ጥበብ', 0.0001135805060129536),\n",
       " ('ጥብብ', 5.570733065839986e-05),\n",
       " ('ጥበቢ', 1.7590755614174365e-05)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given the starting options of ['ጥበ', 'ጥብ', 'ትበ']\n",
    "# What options out of ['ቢ', 'ብ'] are the best next step?\n",
    "best_next_steps(\n",
    "    ['ጥበ', 'ጥብ', 'ትበ'],\n",
    "    ['ቢ', 'ብ']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at ['i', 'ni', 'di', 'hi']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('እኚዲህ', 3.228183081067021e-07),\n",
       " ('እንዲህ', 2.692246230842964e-07),\n",
       " ('እኚድህ', 1.6690833582038836e-07),\n",
       " ('እኚፂህ', 1.57611673325971e-07),\n",
       " ('እንድህ', 1.3919852955185693e-07)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_top_sequences gets the top n options for a breakdown\n",
    "print(\"Looking at\", breakdown)\n",
    "get_top_sequences(breakdown, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at 8 breakdowns\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab92635d768c437984fa1a5a4e26037f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('እኚዲህ', 3.228183081067021e-07),\n",
       " ('እንዲህ', 2.692246230842964e-07),\n",
       " ('እኚድህ', 1.6690833582038836e-07),\n",
       " ('እኚዲህኢ', 2.3856345637625673e-10),\n",
       " ('እኚዲህእ', 2.3244859594548903e-10),\n",
       " ('እንዲህኢ', 1.9895760250176246e-10),\n",
       " ('እንኢዲህ', 1.9895754405773796e-10),\n",
       " ('እንእዲህ', 1.9385786228808685e-10),\n",
       " ('እኚዽኢህ', 1.45560695875896e-10),\n",
       " ('እኚዽእህ', 1.4182976173606308e-10)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_top_sequences_all does it for multiple breakdowns\n",
    "print(\"Looking at\", len(breakdowns), \"breakdowns\")\n",
    "get_top_sequences_all(breakdowns, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cac5ed3eb7540c8ad6e2193ad0c9cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ትበንን', 2.4012266244509255e-07),\n",
       " ('ጥበንን', 1.2694991862853193e-07),\n",
       " ('ትበንኚ', 9.789369922205556e-08),\n",
       " ('ትበንእን', 4.110395324151776e-09),\n",
       " ('ትእበንን', 4.110394925480327e-09)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_transcriptions(\"tibenini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worst-case scenario, totally cleared cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acf2158e758461b87b1f2bdc3897abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=576), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('ያምዣደርጓትኸዊን', 2.2041002574484945e-21),\n",
       " ('ያምዣደርጓጥኸዊን', 1.6694631837741595e-21),\n",
       " ('ያምዣፀርጓትኸዊን', 1.3377501132239324e-21),\n",
       " ('ያምዕዣደርጓትኸዊን', 2.3625769779701323e-23),\n",
       " ('ያምዣደርጓትኸዊንዕ', 2.3625757615350668e-23)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_cached = {}\n",
    "cached = {}\n",
    "\n",
    "# The cache will fill up as we go along, so\n",
    "# this word won't always take so long\n",
    "top_transcriptions(\"yamijadarguatkhawini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "02-OpenNMT training by full sentences.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
