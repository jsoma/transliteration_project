{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import json\n",
    "import string \n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3sbaKBNSNNn"
   },
   "source": [
    "# Read in mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xWKtIPlWgxKf",
    "outputId": "2634151b-beaf-4b66-8917-a86bcc704f60"
   },
   "outputs": [],
   "source": [
    "with open(\"raw/lat_eth.json\") as f:\n",
    "    lat_eth = json.load(f)\n",
    "\n",
    "with open(\"raw/eth_lat.json\") as f:\n",
    "    eth_lat = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OF2nht_ASUV3"
   },
   "source": [
    "# Build corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n4AgAJlpiUfo"
   },
   "outputs": [],
   "source": [
    "# load dictionary to prune transliteration options \n",
    "am_dic_file = open(\"am_dic.txt\", \"r\", encoding=\"utf-8\")\n",
    "am_dic = []\n",
    "\n",
    "for w in am_dic_file.readlines():\n",
    "    am_dic.append(w.rstrip())\n",
    "\n",
    "am_dic_file.close()\n",
    "am_dic = set(am_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vQD-mGaZrA5Y"
   },
   "outputs": [],
   "source": [
    "am_dic_file_2 = open(\"AMH-wiki-tok.txt\", \"r\", encoding=\"utf-8\")\n",
    "am_dic_2 = []\n",
    "\n",
    "for w in am_dic_file_2.readlines():\n",
    "  line = w.rstrip()\n",
    "  words = line.split()\n",
    "  for i in words:\n",
    "    if i in string.punctuation:\n",
    "      words.remove(i)\n",
    "  am_dic_2 += words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_7-IlWTQrY5N",
    "outputId": "2c28dc58-9b57-45df-9a4d-66c8d2bb370c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231333"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "am_dic = am_dic.union(set(am_dic_2))\n",
    "len(am_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ngOAyW6SXw1"
   },
   "source": [
    "## Use CountVectorizer() to do char n grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "wVejetZNSgDR",
    "outputId": "64e4a01f-8f90-42fe-d0bd-6a03c6017bd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/soma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# clean corpus\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "file = open(\"raw/new-am.txt\", \"r\", encoding=\"utf-8\")\n",
    "corpus = file.read()\n",
    "\n",
    "# tokenize corpus (https://machinelearningmastery.com/clean-text-machine-learning-python/)\n",
    "tokens = list(set(word_tokenize(corpus) + am_dic_2))\n",
    "# remove all tokens that are not alphabetic\n",
    "tokens = [word for word in tokens if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T9nWKo1cSl-b"
   },
   "outputs": [],
   "source": [
    "# train model to do ngram work\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(1, 3), analyzer=\"char_wb\")\n",
    "cv_fit = cv.fit_transform(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M6brLQj2S0Sx",
    "outputId": "607060e5-b4d6-4114-9671-b78e7531d557"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 1-3gram:  207249\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size 1-3gram: \", len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "XWBFPHlKS7jh",
    "outputId": "db3cfdd8-4ea6-4898-c5da-a622d2ebb7ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Count:  1585420\n",
      "Bigram Count:  1365220\n",
      "Trigram Count:  1145020\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ngram_list = cv.get_feature_names()\n",
    "count_list = np.asarray(cv_fit.sum(axis=0))[0]\n",
    "\n",
    "# make a dictionary with frequencies \n",
    "freq_dict = dict(zip(ngram_list,count_list))\n",
    "\n",
    "# get unigram, bigram, trigram total counts\n",
    "unigram_count = 0\n",
    "bigram_count = 0\n",
    "trigram_count = 0\n",
    "\n",
    "for key in freq_dict.keys():\n",
    "  if len(key)==1:\n",
    "    unigram_count += freq_dict[key]\n",
    "  elif len(key)==2:\n",
    "    bigram_count += freq_dict[key]\n",
    "  else:\n",
    "    trigram_count += freq_dict[key]\n",
    "\n",
    "print(\"Unigram Count: \", unigram_count)\n",
    "print(\"Bigram Count: \", bigram_count)\n",
    "print(\"Trigram Count: \", trigram_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XYbXyZwgXcSh"
   },
   "source": [
    "## Actual Transliterating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5tlM_qWNgxKl"
   },
   "outputs": [],
   "source": [
    "# generate possible transliterations\n",
    "\n",
    "import string\n",
    "import itertools\n",
    "import operator\n",
    "import string \n",
    "import random\n",
    "\n",
    "# given a sentence in latin characters, splits and sends word by word to the \n",
    "# function transliterate_word\n",
    "def transliterate(text):\n",
    "    sent_trans = []\n",
    "\n",
    "    sentence = [i for j in text.split() for i in (j, ' ')][:-1]\n",
    "    cleaned = []\n",
    "    for elmt in sentence:\n",
    "      elmt_tokenized = word_tokenize(elmt)\n",
    "      if elmt == ' ':\n",
    "        cleaned.append(' ')\n",
    "      elif len(elmt) == len(elmt_tokenized):\n",
    "        cleaned.append(elmt)\n",
    "      else:\n",
    "        for i in elmt_tokenized:\n",
    "          if i == \"'\":\n",
    "            elmt_tokenized.remove(i)\n",
    "        cleaned += elmt_tokenized\n",
    "\n",
    "    for word in cleaned:\n",
    "      sent_trans.append(transliterate_word(word))\n",
    "    \n",
    "    return \"\".join(sent_trans)\n",
    "\n",
    "    \n",
    "# transliterate_word returns spaces/punctuations as appropriate\n",
    "# and sends an actual latin character word to ngram_selected(word) to \n",
    "# obtain the appropriate transliterated word in ethiopic\n",
    "def transliterate_word(word):\n",
    "    if word in string.punctuation and word not in lat_eth.keys():\n",
    "      return word\n",
    "    elif word.isnumeric() == True:\n",
    "      return word\n",
    "    elif word == \" \":\n",
    "      return word\n",
    "    elif len(word) > 15:\n",
    "      return word\n",
    "    else:\n",
    "      word = unidecode.unidecode(word).lower()\n",
    "      return ngram_selected(word)\n",
    "\n",
    "# ngram_selected takes a latin character word and generates all possible ethiopic\n",
    "# transliterations by calling the function possibilities; it then selects the \n",
    "# ethiopic option with the highest score using the function word_score\n",
    "cached_best = {}\n",
    "def ngram_selected(word):\n",
    "    cache_key = word\n",
    "    if cache_key in cached_best.keys():\n",
    "        return cached_best[cache_key]\n",
    "    options = possibilities(word)\n",
    "    if len(options) == 0:\n",
    "      return word\n",
    "    else:\n",
    "      scores = dict()\n",
    "      for opt in options:\n",
    "        score = word_score(opt)\n",
    "        scores[opt] = score\n",
    "      selected_word = max(scores.items(), key=operator.itemgetter(1))[0]\n",
    "      cached_best[cache_key] = selected_word\n",
    "      return selected_word\n",
    "\n",
    "# the function possibilities takes a latin character word and returns all \n",
    "# possible transliterations into ethiopic based on the reverse dictionary\n",
    "# this function calls the function prune to remove entries that are \n",
    "# not in an actual amharic dictionary (unless pruning results in 0 options)   \n",
    "# this function also calls the function convert to go from latin char to \n",
    "# ethiopic char as based on the reverse dictionary\n",
    "def possibilities(word):\n",
    "    # split word into chars\n",
    "    chars = list(word)\n",
    "    \n",
    "    # generate all combinations \n",
    "    # https://stackoverflow.com/questions/27263155/python-find-all-possible-\n",
    "    # word-combinations-with-a-sequence-of-characters-word\n",
    "    combinatorics = itertools.product([True, False], repeat=len(chars) - 1)\n",
    "    latin_segmentation = []\n",
    "    add = True\n",
    "    for combination in combinatorics:\n",
    "        i = 0\n",
    "        one_such_combination = [chars[i]]\n",
    "        for slab in combination:\n",
    "            i += 1\n",
    "            if not slab: # there is a join\n",
    "                one_such_combination[-1] += chars[i]\n",
    "            else:\n",
    "                one_such_combination += [chars[i]]\n",
    "        \n",
    "        for elmt in one_such_combination:\n",
    "            if elmt not in lat_eth.keys():\n",
    "                add = False\n",
    "                break\n",
    "        # only add/consider if segmentation can be converted into ethiopic \n",
    "        # characters\n",
    "        if add == True:  \n",
    "            latin_segmentation.append(one_such_combination)\n",
    "            \n",
    "        # reset\n",
    "        add = True\n",
    "    \n",
    "    # conversion ******************************\n",
    "    ethiopic_opts = []\n",
    "    for segmentation in latin_segmentation:\n",
    "        ethiopic_opts += convert(segmentation)\n",
    "\n",
    "    pruned = prune(ethiopic_opts)\n",
    "    if len(pruned) == 0:\n",
    "      if len(ethiopic_opts) < 100:\n",
    "        return ethiopic_opts\n",
    "      else:\n",
    "        sampling = random.choices(ethiopic_opts, k=99)\n",
    "        return sampling\n",
    "    else:\n",
    "      return pruned\n",
    "\n",
    "\n",
    "# this is called by the function possibilities to convert from latin char\n",
    "# to ethiopic char given a particular segmentation (i.e. i-di vs. i-d-i might\n",
    "# both be sent separately)\n",
    "def convert(segmentation):\n",
    "    final_list = []\n",
    "    relevant_lists = []\n",
    "    for elmt in segmentation:\n",
    "        relevant_lists.append(lat_eth[elmt])\n",
    "    for i in itertools.product(*relevant_lists):\n",
    "        final_list.append(''.join(i))\n",
    "    return final_list\n",
    "\n",
    "# this is called by the function possibilities to prune the list of possible\n",
    "# ethiopic transliterations\n",
    "def prune(possibilities):\n",
    "    final_possibilities = []\n",
    "    for candidate in possibilities: \n",
    "      if candidate in am_dic:\n",
    "          final_possibilities.append(candidate)\n",
    "    return final_possibilities\n",
    "\n",
    "# this function is called by ngram_selected to determine the probability of\n",
    "# an ethiopic word occurring (using ngram counts)\n",
    "# this function calls get_ngrams to split the given word into n-length \n",
    "# subsections for scoring\n",
    "# this function also calls one or multiple of the [n]gram_probability functions \n",
    "# to compute each [n]gram score, which are then weighted evenly in computing \n",
    "# the final score\n",
    "\n",
    "cached = {}\n",
    "\n",
    "def word_score(word):\n",
    "  cache_key = word\n",
    "  if cache_key in cached.keys():\n",
    "    return cached[cache_key]\n",
    "\n",
    "  sequence = list(word)\n",
    "  if len(word) >= 3:\n",
    "    # calculate trigram probability\n",
    "    w = 1/3.0\n",
    "    trigrams = get_ngrams(sequence, 3)\n",
    "    bigrams = get_ngrams(sequence, 2)\n",
    "    unigrams = get_ngrams(sequence, 1)\n",
    "    score_t = trigram_probability(trigrams)\n",
    "    score_b = bigram_probability(bigrams)\n",
    "    score_u = unigram_probability(unigrams)\n",
    "    score = (w*score_t)+(w*score_b)+(w*score_u) \n",
    "  elif len(word) >= 2:\n",
    "    # calculate bigram probability\n",
    "    w = 1/2.0\n",
    "    bigrams = get_ngrams(sequence, 2)\n",
    "    unigrams = get_ngrams(sequence, 1)\n",
    "    score_b = bigram_probability(bigrams)\n",
    "    score_u = unigram_probability(unigrams)\n",
    "    score = (w*score_b)+(w*score_u) \n",
    "  else:\n",
    "    # calculate unigram probability\n",
    "    unigrams = get_ngrams(sequence, 1)\n",
    "    score = unigram_probability(word)\n",
    "\n",
    "  cached[cache_key] = score\n",
    "  return score\n",
    "\n",
    "# called by the function word_score to generate n gram subsections \n",
    "# from a given ethiopic word\n",
    "def get_ngrams(sequence, n):\n",
    "    input = sequence\n",
    "    output = []\n",
    "    for i in range(0, len(input) - n + 1):\n",
    "        output.append(input[i:i + n])\n",
    "    \n",
    "    return [''.join(l) for l in output]\n",
    "\n",
    "# these funtions are called by word_score to compute [n]gram probabilities given\n",
    "# an ethiopic word \n",
    "def trigram_probability(trigrams):\n",
    "  freq = 0\n",
    "  for t in trigrams:\n",
    "    # get freq\n",
    "    if t in freq_dict:\n",
    "      freq += freq_dict[t]\n",
    "  avg_prob = freq/(len(trigrams) * trigram_count)\n",
    "  return avg_prob\n",
    "\n",
    "def bigram_probability(bigrams):\n",
    "  freq = 0\n",
    "  for b in bigrams:\n",
    "    # get freq\n",
    "    if b in freq_dict:\n",
    "      freq += freq_dict[b]\n",
    "  avg_prob = freq/(len(bigrams) * bigram_count)\n",
    "  return avg_prob\n",
    "\n",
    "def unigram_probability(unigrams):\n",
    "  freq = 0\n",
    "  for u in unigrams:\n",
    "    # get freq\n",
    "    if u in freq_dict:\n",
    "      freq += freq_dict[u]\n",
    "  avg_prob = freq/(len(unigrams) * unigram_count)\n",
    "  return avg_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gVLCO1_WYkuA",
    "outputId": "95627755-7147-4d10-86d4-426aa4f4d700"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'እንድህ ስል ። ሁለት ስዎች ሊጸልዩ ወደ'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transliterate(\"inidihi sil . hulat sewochi liseliyu wade\")\n",
    "#እንዲህ ሲል ። ሁለት ሰዎች ሊጸልዩ ወደ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xc_m4oWrZD9q",
    "outputId": "6d0f2c8d-ae91-48d0-87a2-964726891b1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'እየሱስም ። እውንት እውነት እላችኋለሁ'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transliterate(\"ijasusme . 'eweneti iwnat `elacehualehu\")\n",
    "#ኢየሱስም ። እውነት እውነት እላችኋለሁ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OKLZwtMwZwJy",
    "outputId": "7ef25a78-5c57-453c-e867-8bd219cf6a3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'አባቱን ውይም እናቱን አያከብርም ትላላችሁ ፤'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transliterate(\"abatune wejeme ina'tun ajakebirm tlalacihu ;\")\n",
    "#አባቱን ወይም እናቱን አያከብርም ትላላችሁ ፤ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Mn9z8eBgZ9gQ",
    "outputId": "1bd92c45-1f43-47d5-b09e-8df5810b3974"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'በምን አይነትም ሞት ይሞት ዘንድ እንዳለው'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transliterate(\"bamine 'ajenetim mote ymoti zendi `enidalawe\")\n",
    "#በምን ዓይነትም ሞት ይሞት ዘንድ እንዳለው"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9xIKPBjwBx3R"
   },
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "-Exg0aSnYR1H",
    "outputId": "63c80f34-fd41-4de4-a0f7-dca44ff57a7b"
   },
   "outputs": [],
   "source": [
    "#!pip install python-levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b32ewnOKWAPc"
   },
   "outputs": [],
   "source": [
    "# EVALUATION\n",
    "\n",
    "import Levenshtein\n",
    "from statistics import mean\n",
    "\n",
    "# this function takes two files (parallel texts) and generates\n",
    "# two lists after stripping beginning/trailing whitespace\n",
    "def extract_from_files(ethiopic_file, latin_file):\n",
    "  eth_file = open(ethiopic_file)\n",
    "  lat_file = open(latin_file)\n",
    "  ethiopic = [line.rstrip() for line in eth_file.readlines()]\n",
    "  latin = [line.rstrip() for line in lat_file.readlines()] \n",
    "  \n",
    "  ethiopic = list(ethiopic)\n",
    "  latin = list(latin)\n",
    "\n",
    "  return ethiopic, latin\n",
    "\n",
    "# this function takes two parallel lists and evaluates how\n",
    "# our model performs\n",
    "def evaluate(ethiopic, latin):\n",
    "  accuracies = []\n",
    "  count = 0\n",
    "  total_length = 0\n",
    "\n",
    "  for line in ethiopic:\n",
    "    total_length += len(line)\n",
    "\n",
    "  for line in tqdm_notebook(latin):\n",
    "    predict = transliterate(line)\n",
    "    correct = ethiopic[count]\n",
    "    weight = len(correct) / total_length\n",
    "    accuracy = Levenshtein.ratio(predict,correct) * weight\n",
    "    accuracies.append(accuracy) \n",
    "    count += 1\n",
    "\n",
    "  return sum(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uI7hF_ASVpod"
   },
   "source": [
    "## first on google translated pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CaB6cgFgB0m3"
   },
   "outputs": [],
   "source": [
    "ethiopic_tot, latin_tot = extract_from_files(\"raw/original.txt\", \"raw/transliterated.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcggmxtcbDMk"
   },
   "outputs": [],
   "source": [
    "ethiopic_2 = ethiopic_tot[0:10]\n",
    "latin_2 = latin_tot[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "xNN6j0IBbKQ_",
    "outputId": "4e3a116b-8718-488c-f2f7-3c87374d6b03"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a18f16f67ac49a58dfa4dc8ebd993c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7700315602130576"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ethiopic_2, latin_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zq_HhehoblYF"
   },
   "outputs": [],
   "source": [
    "ethiopic_3 = ethiopic_tot[0:100]\n",
    "latin_3 = latin_tot[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "6TqMTQoobriB",
    "outputId": "2f65dd9f-0587-4ee0-93e6-5b37de2546b5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09f780d5594421a83a9eb767ba6b3ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-4562d69967b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0methiopic_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatin_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-13d086d65eb5>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(ethiopic, latin)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransliterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0methiopic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-493f0e808e11>\u001b[0m in \u001b[0;36mtransliterate\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcleaned\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m       \u001b[0msent_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransliterate_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_trans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-493f0e808e11>\u001b[0m in \u001b[0;36mtransliterate_word\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munidecode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munidecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mngram_selected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# ngram_selected takes a latin character word and generates all possible ethiopic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-493f0e808e11>\u001b[0m in \u001b[0;36mngram_selected\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcache_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcached_best\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcached_best\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpossibilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-493f0e808e11>\u001b[0m in \u001b[0;36mpossibilities\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0methiopic_opts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegmentation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mpruned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0methiopic_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpruned\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0methiopic_opts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-493f0e808e11>\u001b[0m in \u001b[0;36mprune\u001b[0;34m(possibilities)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mfinal_possibilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcandidate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibilities\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mcandidate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mam_dic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m           \u001b[0mfinal_possibilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_possibilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate(ethiopic_3, latin_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "6mzEJ8ZRQAmQ",
    "outputId": "c0af2674-f434-4a16-b010-9862a8938ef8"
   },
   "outputs": [],
   "source": [
    "ethiopic_large = ethiopic_tot[0:1000]\n",
    "latin_large = latin_tot[0:1000]\n",
    "evaluate(ethiopic_large,latin_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mtBrMuWyVtiN"
   },
   "source": [
    "## now on manually generated parallel texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4e6uVXTAN-7M",
    "outputId": "6f3f1a5a-6d71-4227-e806-725671a31cdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4663842474737726"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethiopic_taitu, latin_taitu = extract_from_files('taitu_am.txt','taitu_rom.txt')\n",
    "taitu_score = evaluate(ethiopic_taitu, latin_taitu)\n",
    "taitu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "YNQSeFas_DRE",
    "outputId": "55da7ed9-8392-4d5d-d5bf-f0c85f263c11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6770958851877996"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethiopic_sera, latin_sera = extract_from_files('sera_am.txt','sera_rom.txt')\n",
    "sera_score = evaluate(ethiopic_sera, latin_sera)\n",
    "sera_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "QCeRoMEQBHBE",
    "outputId": "a23b3eb7-4a8b-4481-f82c-2597e697dfbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7004329720057354"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethiopic_tewodros, latin_tewodros = extract_from_files('tewodros_am.txt','tewodros_rom.txt')\n",
    "tewodros_score = evaluate(ethiopic_tewodros, latin_tewodros)\n",
    "tewodros_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "K08N7ygzC017",
    "outputId": "dbebea68-1b51-429f-88ee-f8641b2028c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6854984422936918"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethiopic_prayer, latin_prayer = extract_from_files('lords_prayer_am.txt',\n",
    "                                                   'lords_prayer_rom.txt')\n",
    "prayer_score = evaluate(ethiopic_prayer, latin_prayer)\n",
    "prayer_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "h5kXuF9_BRWg",
    "outputId": "dd88e8e5-df91-4813-f058-9e904937e3e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5501152019011929"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethiopic_yasstesseriyal, latin_yasstesseriyal = extract_from_files('yasstesseriyal_am.txt',\n",
    "                                                   'yasstesseriyal_rom.txt')\n",
    "yasstesseriyal_score = evaluate(ethiopic_yasstesseriyal, latin_yasstesseriyal)\n",
    "yasstesseriyal_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QGtxEjbVEIuJ",
    "outputId": "c9b7be8a-fcf9-410a-b4bd-3c4ac3fedcf0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6159053497724385"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_manual = mean([sera_score, yasstesseriyal_score, tewodros_score, \n",
    "                      prayer_score, taitu_score])\n",
    "average_manual"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "pk_work_5/7.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
