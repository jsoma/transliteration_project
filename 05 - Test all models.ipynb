{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ORy8sSAawbp-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import Levenshtein\n",
    "from statistics import mean\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import unidecode\n",
    "import json\n",
    "import string \n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f_CqDe3Z1VO_"
   },
   "outputs": [],
   "source": [
    "with open('lat_eth.json') as f:\n",
    "    lat_eth = json.load(f)\n",
    "\n",
    "with open('eth_lat.json') as f:\n",
    "    eth2lat = json.load(f)\n",
    "\n",
    "with open('char2idx.json') as f:\n",
    "    char2idx = json.load(f)\n",
    "\n",
    "with open('idx2char.json') as f:\n",
    "    idx2char = np.array(json.load(f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character n-grams\n",
    "\n",
    "## Build corpus\n",
    "\n",
    "Read in Amharic dictionary and Amharic Wikipedia, combine into corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231333"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dictionary to prune transliteration options \n",
    "am_dic_file = open(\"am_dic.txt\", \"r\", encoding=\"utf-8\")\n",
    "am_dic = []\n",
    "\n",
    "for w in am_dic_file.readlines():\n",
    "    am_dic.append(w.rstrip())\n",
    "\n",
    "am_dic_file.close()\n",
    "am_dic = set(am_dic)\n",
    "\n",
    "# Read in Wikipedia\n",
    "am_dic_file_2 = open(\"AMH-wiki-tok.txt\", \"r\", encoding=\"utf-8\")\n",
    "am_dic_2 = []\n",
    "\n",
    "for w in am_dic_file_2.readlines():\n",
    "  line = w.rstrip()\n",
    "  words = line.split()\n",
    "  for i in words:\n",
    "    if i in string.punctuation:\n",
    "      words.remove(i)\n",
    "  am_dic_2 += words\n",
    "\n",
    "am_dic = am_dic.union(set(am_dic_2))\n",
    "len(am_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute char n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/soma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# clean corpus\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "file = open(\"raw/new-am.txt\", \"r\", encoding=\"utf-8\")\n",
    "corpus = file.read()\n",
    "\n",
    "# tokenize corpus (https://machinelearningmastery.com/clean-text-machine-learning-python/)\n",
    "tokens = list(set(word_tokenize(corpus) + am_dic_2))\n",
    "# remove all tokens that are not alphabetic\n",
    "tokens = [word for word in tokens if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 1-3gram:  207249\n"
     ]
    }
   ],
   "source": [
    "# train model to do ngram work\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(1, 3), analyzer=\"char_wb\")\n",
    "cv_fit = cv.fit_transform(tokens)\n",
    "\n",
    "print(\"Vocabulary size 1-3gram: \", len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Count:  1585420\n",
      "Bigram Count:  1365220\n",
      "Trigram Count:  1145020\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ngram_list = cv.get_feature_names()\n",
    "count_list = np.asarray(cv_fit.sum(axis=0))[0]\n",
    "\n",
    "# make a dictionary with frequencies \n",
    "freq_dict = dict(zip(ngram_list,count_list))\n",
    "\n",
    "# get unigram, bigram, trigram total counts\n",
    "unigram_count = 0\n",
    "bigram_count = 0\n",
    "trigram_count = 0\n",
    "\n",
    "for key in freq_dict.keys():\n",
    "  if len(key)==1:\n",
    "    unigram_count += freq_dict[key]\n",
    "  elif len(key)==2:\n",
    "    bigram_count += freq_dict[key]\n",
    "  else:\n",
    "    trigram_count += freq_dict[key]\n",
    "\n",
    "print(\"Unigram Count: \", unigram_count)\n",
    "print(\"Bigram Count: \", bigram_count)\n",
    "print(\"Trigram Count: \", trigram_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transliteration tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate possible transliterations\n",
    "\n",
    "import string\n",
    "import itertools\n",
    "import operator\n",
    "import string \n",
    "import random\n",
    "\n",
    "# given a sentence in latin characters, splits and sends word by word to the \n",
    "# function transliterate_word\n",
    "def char_ngram_transliterate(text):\n",
    "    sent_trans = []\n",
    "\n",
    "    sentence = [i for j in text.split() for i in (j, ' ')][:-1]\n",
    "    cleaned = []\n",
    "    for elmt in sentence:\n",
    "      elmt_tokenized = word_tokenize(elmt)\n",
    "      if elmt == ' ':\n",
    "        cleaned.append(' ')\n",
    "      elif len(elmt) == len(elmt_tokenized):\n",
    "        cleaned.append(elmt)\n",
    "      else:\n",
    "        for i in elmt_tokenized:\n",
    "          if i == \"'\":\n",
    "            elmt_tokenized.remove(i)\n",
    "        cleaned += elmt_tokenized\n",
    "\n",
    "    for word in cleaned:\n",
    "      sent_trans.append(char_ngram_transliterate_word(word))\n",
    "    \n",
    "    return \"\".join(sent_trans)\n",
    "\n",
    "    \n",
    "# transliterate_word returns spaces/punctuations as appropriate\n",
    "# and sends an actual latin character word to ngram_selected(word) to \n",
    "# obtain the appropriate transliterated word in ethiopic\n",
    "def char_ngram_transliterate_word(word):\n",
    "    if word in string.punctuation and word not in lat_eth.keys():\n",
    "      return word\n",
    "    elif word.isnumeric() == True:\n",
    "      return word\n",
    "    elif word == \" \":\n",
    "      return word\n",
    "    elif len(word) > 15:\n",
    "      return word\n",
    "    else:\n",
    "      word = unidecode.unidecode(word).lower()\n",
    "      return ngram_selected(word)\n",
    "\n",
    "# ngram_selected takes a latin character word and generates all possible ethiopic\n",
    "# transliterations by calling the function possibilities; it then selects the \n",
    "# ethiopic option with the highest score using the function word_score\n",
    "char_ngram_cached_best = {}\n",
    "def ngram_selected(word):\n",
    "    cache_key = word\n",
    "    if cache_key in char_ngram_cached_best.keys():\n",
    "        return char_ngram_cached_best[cache_key]\n",
    "    options = possibilities(word)\n",
    "    if len(options) == 0:\n",
    "      return word\n",
    "    else:\n",
    "      scores = dict()\n",
    "      for opt in options:\n",
    "        score = word_score(opt)\n",
    "        scores[opt] = score\n",
    "      selected_word = max(scores.items(), key=operator.itemgetter(1))[0]\n",
    "#       char_ngram_cached_best[cache_key] = selected_word\n",
    "      return selected_word\n",
    "\n",
    "# the function possibilities takes a latin character word and returns all \n",
    "# possible transliterations into ethiopic based on the reverse dictionary\n",
    "# this function calls the function prune to remove entries that are \n",
    "# not in an actual amharic dictionary (unless pruning results in 0 options)   \n",
    "# this function also calls the function convert to go from latin char to \n",
    "# ethiopic char as based on the reverse dictionary\n",
    "def possibilities(word):\n",
    "    # split word into chars\n",
    "    chars = list(word)\n",
    "    \n",
    "    # generate all combinations \n",
    "    # https://stackoverflow.com/questions/27263155/python-find-all-possible-\n",
    "    # word-combinations-with-a-sequence-of-characters-word\n",
    "    combinatorics = itertools.product([True, False], repeat=len(chars) - 1)\n",
    "    latin_segmentation = []\n",
    "    add = True\n",
    "    for combination in combinatorics:\n",
    "        i = 0\n",
    "        one_such_combination = [chars[i]]\n",
    "        for slab in combination:\n",
    "            i += 1\n",
    "            if not slab: # there is a join\n",
    "                one_such_combination[-1] += chars[i]\n",
    "            else:\n",
    "                one_such_combination += [chars[i]]\n",
    "        \n",
    "        for elmt in one_such_combination:\n",
    "            if elmt not in lat_eth.keys():\n",
    "                add = False\n",
    "                break\n",
    "        # only add/consider if segmentation can be converted into ethiopic \n",
    "        # characters\n",
    "        if add == True:  \n",
    "            latin_segmentation.append(one_such_combination)\n",
    "            \n",
    "        # reset\n",
    "        add = True\n",
    "    \n",
    "    # conversion ******************************\n",
    "    ethiopic_opts = []\n",
    "    for segmentation in latin_segmentation:\n",
    "        ethiopic_opts += convert(segmentation)\n",
    "\n",
    "    pruned = prune(ethiopic_opts)\n",
    "    if len(pruned) == 0:\n",
    "      if len(ethiopic_opts) < 100:\n",
    "        return ethiopic_opts\n",
    "      else:\n",
    "        sampling = random.choices(ethiopic_opts, k=99)\n",
    "        return sampling\n",
    "    else:\n",
    "      return pruned\n",
    "\n",
    "\n",
    "# this is called by the function possibilities to convert from latin char\n",
    "# to ethiopic char given a particular segmentation (i.e. i-di vs. i-d-i might\n",
    "# both be sent separately)\n",
    "def convert(segmentation):\n",
    "    final_list = []\n",
    "    relevant_lists = []\n",
    "    for elmt in segmentation:\n",
    "        relevant_lists.append(lat_eth[elmt])\n",
    "    for i in itertools.product(*relevant_lists):\n",
    "        final_list.append(''.join(i))\n",
    "    return final_list\n",
    "\n",
    "# this is called by the function possibilities to prune the list of possible\n",
    "# ethiopic transliterations\n",
    "def prune(possibilities):\n",
    "    final_possibilities = []\n",
    "    for candidate in possibilities: \n",
    "      if candidate in am_dic:\n",
    "          final_possibilities.append(candidate)\n",
    "    return final_possibilities\n",
    "\n",
    "# this function is called by ngram_selected to determine the probability of\n",
    "# an ethiopic word occurring (using ngram counts)\n",
    "# this function calls get_ngrams to split the given word into n-length \n",
    "# subsections for scoring\n",
    "# this function also calls one or multiple of the [n]gram_probability functions \n",
    "# to compute each [n]gram score, which are then weighted evenly in computing \n",
    "# the final score\n",
    "\n",
    "\n",
    "def word_score(word):\n",
    "  sequence = list(word)\n",
    "  if len(word) >= 3:\n",
    "    # calculate trigram probability\n",
    "    w = 1/3.0\n",
    "    trigrams = get_ngrams(sequence, 3)\n",
    "    bigrams = get_ngrams(sequence, 2)\n",
    "    unigrams = get_ngrams(sequence, 1)\n",
    "    score_t = trigram_probability(trigrams)\n",
    "    score_b = bigram_probability(bigrams)\n",
    "    score_u = unigram_probability(unigrams)\n",
    "    score = (w*score_t)+(w*score_b)+(w*score_u) \n",
    "  elif len(word) >= 2:\n",
    "    # calculate bigram probability\n",
    "    w = 1/2.0\n",
    "    bigrams = get_ngrams(sequence, 2)\n",
    "    unigrams = get_ngrams(sequence, 1)\n",
    "    score_b = bigram_probability(bigrams)\n",
    "    score_u = unigram_probability(unigrams)\n",
    "    score = (w*score_b)+(w*score_u) \n",
    "  else:\n",
    "    # calculate unigram probability\n",
    "    unigrams = get_ngrams(sequence, 1)\n",
    "    score = unigram_probability(word)\n",
    "\n",
    "  return score\n",
    "\n",
    "# called by the function word_score to generate n gram subsections \n",
    "# from a given ethiopic word\n",
    "def get_ngrams(sequence, n):\n",
    "    input = sequence\n",
    "    output = []\n",
    "    for i in range(0, len(input) - n + 1):\n",
    "        output.append(input[i:i + n])\n",
    "    \n",
    "    return [''.join(l) for l in output]\n",
    "\n",
    "# these funtions are called by word_score to compute [n]gram probabilities given\n",
    "# an ethiopic word \n",
    "def trigram_probability(trigrams):\n",
    "  freq = 0\n",
    "  for t in trigrams:\n",
    "    # get freq\n",
    "    if t in freq_dict:\n",
    "      freq += freq_dict[t]\n",
    "  avg_prob = freq/(len(trigrams) * trigram_count)\n",
    "  return avg_prob\n",
    "\n",
    "def bigram_probability(bigrams):\n",
    "  freq = 0\n",
    "  for b in bigrams:\n",
    "    # get freq\n",
    "    if b in freq_dict:\n",
    "      freq += freq_dict[b]\n",
    "  avg_prob = freq/(len(bigrams) * bigram_count)\n",
    "  return avg_prob\n",
    "\n",
    "def unigram_probability(unigrams):\n",
    "  freq = 0\n",
    "  for u in unigrams:\n",
    "    # get freq\n",
    "    if u in freq_dict:\n",
    "      freq += freq_dict[u]\n",
    "  avg_prob = freq/(len(unigrams) * unigram_count)\n",
    "  return avg_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'እንድህ ስል ። ሁለት ስዎች ሊጸልዩ ወደ'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_ngram_transliterate(\"inidihi sil . hulat sewochi liseliyu wade\")\n",
    "#እንዲህ ሲል ። ሁለት ሰዎች ሊጸልዩ ወደ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'እየሱስም ። እውንት እውነት እላችኋለሁ'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_ngram_transliterate(\"ijasusme . 'eweneti iwnat `elacehualehu\")\n",
    "#ኢየሱስም ። እውነት እውነት እላችኋለሁ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'አባቱን ውይም እናቱን አያከብርም ትላላችሁ ፤'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_ngram_transliterate(\"abatune wejeme ina'tun ajakebirm tlalacihu ;\")\n",
    "#አባቱን ወይም እናቱን አያከብርም ትላላችሁ ፤ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'በምን አይነትም ሞት ይሞት ዘንድ እንዳለው'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_ngram_transliterate(\"bamine 'ajenetim mote ymoti zendi `enidalawe\")\n",
    "#በምን ዓይነትም ሞት ይሞት ዘንድ እንዳለው"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word n-gram model\n",
    "\n",
    "## Build corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = [\"፤\", \".\", \"»\", \"(\", \")\", \"/\", \"።\", \"’\", \"“\", \"፣\", \"!\", \"”\", \"‘\",\\\n",
    "               \"፦\", \"’\", \"፡\", \"&\", \"፥\", \"፧\"]\n",
    "def tokenize(word): \n",
    "  clean_word = \"\"\n",
    "  for n in range(len(word)):\n",
    "    if word[n] in punctuation:\n",
    "      clean_word += \" \"\n",
    "    elif word[n].isdigit():\n",
    "      clean_word += \" \"\n",
    "    else:\n",
    "      clean_word += word[n]\n",
    "  \n",
    "  clean_word = clean_word.strip()\n",
    "  clean_words = clean_word.split()\n",
    "  return clean_words\n",
    "\n",
    "new_am = open(\"raw/new-am.txt\", \"r\")\n",
    "corpus = open(\"corpus.txt\", \"w\") # corpus = cleaned new_am\n",
    "\n",
    "unique_words = set()\n",
    "\n",
    "for line in new_am.readlines():\n",
    "  words = line.split()\n",
    "  clean_words = []\n",
    "  for n in range(len(words)):\n",
    "    word = words[n]\n",
    "    if word.isdigit():\n",
    "      continue\n",
    "    else: \n",
    "      word_tokenized = tokenize(word)\n",
    "      for w in word_tokenized:\n",
    "        clean_words.append(w)\n",
    "        unique_words.add(w)\n",
    "  corpus.write(\" \".join(clean_words))\n",
    "  corpus.write(\"\\n\")\n",
    "\n",
    "corpus.close()\n",
    "new_am.close()\n",
    "\n",
    "corpus_dic = open(\"corpus_dic.txt\", \"w\")\n",
    "for word in unique_words:\n",
    "  corpus_dic.write(word)\n",
    "  corpus_dic.write(\"\\n\")\n",
    "corpus_dic.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def extract_ngrams(corpus_file):\n",
    "  corpus = open(corpus_file, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "  uni_dict = {}\n",
    "  bi_dict = {}\n",
    "  tri_dict = {}\n",
    "  ngram_totals = {'uni': 0, 'bi': 0, 'tri': 0}\n",
    "  for line in corpus.readlines():\n",
    "    words = line.split()\n",
    "\n",
    "    # Compute ngrams\n",
    "    unigrams = ngrams(words, 1)\n",
    "    bigrams = ngrams(words, 2)\n",
    "    trigrams = ngrams(words, 3)\n",
    "\n",
    "    # Calculate frequency of each ngram unit\n",
    "    for uni_tuple in unigrams:\n",
    "      uni = uni_tuple[0]\n",
    "      if uni in uni_dict:\n",
    "        uni_dict[uni] += 1\n",
    "      else: \n",
    "        uni_dict[uni] = 1\n",
    "      ngram_totals['uni'] += 1\n",
    "      \n",
    "    for bi in bigrams:\n",
    "      if bi in bi_dict:\n",
    "        bi_dict[bi] += 1\n",
    "      else:\n",
    "        bi_dict[bi] = 1\n",
    "      ngram_totals['bi'] += 1\n",
    "\n",
    "    for tri in trigrams:\n",
    "      if tri in tri_dict:\n",
    "        tri_dict[tri] += 1\n",
    "      else:\n",
    "        tri_dict[tri] = 1\n",
    "      ngram_totals['tri'] += 1\n",
    "\n",
    "  return uni_dict, bi_dict, tri_dict, ngram_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soma/.pyenv/versions/3.6.8/lib/python3.6/site-packages/ipykernel_launcher.py:34: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "/Users/soma/.pyenv/versions/3.6.8/lib/python3.6/site-packages/ipykernel_launcher.py:27: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    }
   ],
   "source": [
    "uni_dict, bi_dict, tri_dict, ngram_totals = extract_ngrams(\"corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram dictionary: 119481\n",
      "bigram dictionary: 492127\n",
      "trigram dictionary: 601817\n"
     ]
    }
   ],
   "source": [
    "print(\"unigram dictionary:\", len(uni_dict))\n",
    "print(\"bigram dictionary:\", len(bi_dict))\n",
    "print(\"trigram dictionary:\", len(tri_dict))\n",
    "\n",
    "ngram_dict = [uni_dict, bi_dict, tri_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ngram_prob(ngram_dict, ngram_totals, vocab_dict):\n",
    "  trigrams = ngram_dict[2]\n",
    "  bigrams = ngram_dict[1]\n",
    "  unigrams = ngram_dict[0]\n",
    "\n",
    "  v = len(vocab_dict)\n",
    "\n",
    "  # trigram prob\n",
    "  tri_prob_dict = {}\n",
    "  for (tri, frq) in trigrams.items():\n",
    "    b = tri[0], tri[1]\n",
    "    prob = frq + 1/ bigrams[b] + v # add 1 smoothing\n",
    "    tri_prob_dict[tri] = prob\n",
    "  \n",
    "  # bigram prob\n",
    "  bi_prob_dict = {}\n",
    "  for (bi, frq) in bigrams.items():\n",
    "    u = bi[0]\n",
    "    prob = frq / unigrams[u]\n",
    "    bi_prob_dict[bi] = prob\n",
    "\n",
    "  # unigram prob\n",
    "  uni_prob_dict = {}\n",
    "  for (uni, frq) in unigrams.items():\n",
    "    prob = frq / ngram_totals['uni']\n",
    "    uni_prob_dict[uni] = prob\n",
    "\n",
    "  return uni_prob_dict, bi_prob_dict, tri_prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_prob_dict, bi_prob_dict, tri_prob_dict = calculate_ngram_prob(ngram_dict, ngram_totals, unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transliteration tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_probabilities(sen, ngram_dict_probabilities):\n",
    "  probability = {}\n",
    "  token = sen.split()\n",
    "\n",
    "  # trigram prob\n",
    "  tri_dict_prob = ngram_dict_probabilities[2]\n",
    "  if len(token) >= 3: \n",
    "    tri = (token[-3], token[-2], token[-1])\n",
    "    if tri in tri_dict_prob:\n",
    "      probability[tri] = tri_dict_prob[tri]\n",
    "  \n",
    "  # bigram prob\n",
    "  bi_dict_prob = ngram_dict_probabilities[1]\n",
    "  if len(token) >= 2:\n",
    "    bi = (token[-2], token[-1])\n",
    "    if bi in bi_dict_prob:\n",
    "      probability[bi] = bi_dict_prob[bi]\n",
    "\n",
    "  # unigram prob\n",
    "  uni_dict_prob = ngram_dict_probabilities[0]\n",
    "  if len(token) >= 1:\n",
    "    uni  = token[-1]\n",
    "    if uni in uni_dict_prob:\n",
    "      probability = uni_dict_prob[uni]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "# Takes in a text of sentence and returns the highest likelihood\n",
    "# transliterated sequence.\n",
    "def word_ngram_transliterate(sentence):\n",
    "  tokens = sentence.split()\n",
    "  transliterated_tokens = {}\n",
    "  for token in tokens:\n",
    "    all_candidates = possibilities(token)\n",
    "    valid_candidates = prune(all_candidates)\n",
    "    if len(valid_candidates) == 0 and len(all_candidates) == 0:\n",
    "      print(token)\n",
    "    if token not in transliterated_tokens:\n",
    "      transliterated_tokens[token] = valid_candidates\n",
    "\n",
    "  transliterated_sentence = calculate_best_sequence(tokens, transliterated_tokens)\n",
    "\n",
    "  return transliterated_sentence\n",
    "\n",
    "# Takes a list of words in a sentence and a list of all the words' possible\n",
    "# candidates and determines the best sequence of transliterated words.\n",
    "def calculate_best_sequence(tokens, transliterated_tokens):\n",
    "  # Create all possible sentences.\n",
    "  sentence_candidates = create_all_sentences(tokens, transliterated_tokens)\n",
    "\n",
    "  # Score all possible sentences. \n",
    "  all_scores = {}\n",
    "  for sentence in sentence_candidates:\n",
    "    score = calculate_sentence_score(sentence)\n",
    "    all_scores[sentence] = score\n",
    "\n",
    "  if len(sentence_candidates) == 0:\n",
    "    return \" \"\n",
    "\n",
    "  return max(all_scores.items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "# Creates all possible sequences of words and returns a list. \n",
    "def create_all_sentences(tokens, transliterated_tokens):\n",
    "  possible_sentences = {}\n",
    "  for n in range(len(tokens)): \n",
    "    if n == 0:\n",
    "      possible_sentences[tokens[n]] = [t for t in transliterated_tokens[tokens[n]]]\n",
    "    else: \n",
    "      for t in possible_sentences[tokens[n-1]]:\n",
    "        for next_t in transliterated_tokens[tokens[n]]:\n",
    "          new_t = t + \" \" + next_t\n",
    "          if tokens[n] in possible_sentences:\n",
    "            possible_sentences[tokens[n]].append(new_t)\n",
    "          else:\n",
    "            possible_sentences[tokens[n]] = [new_t]\n",
    "      # If transliterated_tokens[tokens[n]] is empty add content\n",
    "      # from tokens[n-1]\n",
    "      if possible_sentences.get(tokens[n]) is None:\n",
    "        possible_sentences[tokens[n]] = possible_sentences[tokens[n-1]]\n",
    "  return possible_sentences[tokens[len(tokens)-1]]\n",
    "\n",
    "\n",
    "def calculate_ngram_score(ngram, ngram_dict_prob):\n",
    "  ngram_list = list(ngram)\n",
    "  avg_ngram_score = 0\n",
    "  for ngram in ngram_list:\n",
    "    if ngram in ngram_dict_prob:\n",
    "      avg_ngram_score = ngram_dict_prob[ngram]\n",
    "  return avg_ngram_score/len(ngram_list)\n",
    "\n",
    "# Takes a sentence (sequential list of words) and returns a likelihood score\n",
    "# based on its ngram sequences.\n",
    "def calculate_sentence_score(sentence):\n",
    "  unigrams = ngrams(sentence, 1)\n",
    "  bigrams = ngrams(sentence, 2)\n",
    "  trigrams = ngrams(sentence, 3)\n",
    "\n",
    "  score = 0\n",
    "  tri_weight = 2.0\n",
    "  bi_weight = 1.0\n",
    "  uni_weight = 0.5\n",
    "\n",
    "  trigram_score = tri_weight * calculate_ngram_score(trigrams, tri_prob_dict)\n",
    "  bigram_score = bi_weight * calculate_ngram_score(bigrams, bi_prob_dict)\n",
    "  unigram_score = uni_weight * calculate_ngram_score(unigrams, uni_prob_dict)\n",
    "\n",
    "  score += trigram_score + bigram_score + unigram_score\n",
    "  score = score / len(sentence)\n",
    "\n",
    "  return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m m q', 'm m r', 'm m s', 'm o q', 'm o r', 'm o s', 'n m q', 'n m r', 'n m s', 'n o q', 'n o r', 'n o s']\n"
     ]
    }
   ],
   "source": [
    "tokens = [\"a\", \"b\", \"c\"]\n",
    "tt = {\"a\": [\"m\", \"n\"], \"b\":[\"m\", \"o\"], \"c\":[\"q\", \"r\", \"s\"]}\n",
    "\n",
    "print(create_all_sentences(tokens, tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'እንዲህ ሲል ። ሁለት ሰዎች ሊጸልዩ ወደ'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ngram_transliterate(\"inidihi sil . hulat sewochi liseliyu wade\")\n",
    "#እንዲህ ሲል ። ሁለት ሰዎች ሊጸልዩ ወደ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "6UNFupcc6LQn",
    "outputId": "62723844-b11d-4751-f2e0-7fdae3026d07"
   },
   "outputs": [],
   "source": [
    "# Used a custom loss function so we need to load_model without compilation\n",
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model = tf.keras.models.load_model(\"char_model\", compile=False)\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transliteration tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_prob_cached = {}\n",
    "cont_cached = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to probabilities\n",
    "\n",
    "def get_next_char_probs(start_string):\n",
    "    cache_key = start_string\n",
    "    if cache_key in cont_cached.keys():\n",
    "        pred_prob = cont_cached[cache_key]\n",
    "    else:\n",
    "        input_eval = [char2idx[s] for s in start_string]\n",
    "        input_eval = tf.expand_dims(input_eval, 0)\n",
    "        # print(\"input_eval is\", input_eval)\n",
    "        # Ask model to evaluate what's next, given start_string\n",
    "        model.reset_states()\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        # Convert scores to probabilities\n",
    "        pred_prob = tf.nn.softmax(predictions[0])\n",
    "\n",
    "        # Cache probabilities\n",
    "        cont_cached[cache_key] = pred_prob\n",
    "    return pred_prob\n",
    "\n",
    "def continuation_proba(start_string, next_char):\n",
    "    try:\n",
    "        # Index of target for when we pull it out of the scores\n",
    "        target_idx = char2idx[next_char]\n",
    "    except:\n",
    "        # If we've never seen that char before, get out of here!\n",
    "        return 0\n",
    "    \n",
    "    # Check the cache\n",
    "    cache_key = start_string\n",
    "    if cache_key in cont_cached.keys():\n",
    "        pred_prob = cont_cached[cache_key]\n",
    "    else:\n",
    "        # Convert characters into indices for tensorflow processing\n",
    "        try:\n",
    "            input_eval = [char2idx[s] for s in start_string]\n",
    "        except:\n",
    "            # We've never seen it!!! just exit!!\n",
    "            return 0\n",
    "\n",
    "        input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "        # Ask model to evaluate what's next, given start_string\n",
    "        model.reset_states()\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        # Convert scores to probabilities\n",
    "        pred_prob = tf.nn.softmax(predictions[0])\n",
    "\n",
    "        # Cache probabilities\n",
    "        cont_cached[cache_key] = pred_prob\n",
    "\n",
    "    \n",
    "    proba = pred_prob[target_idx].numpy()\n",
    "    return proba\n",
    "\n",
    "def text_proba(text, length_adj=False):\n",
    "    # Check the cache\n",
    "    cache_key = text + (\"-adj\" if length_adj else \"-unadj\")\n",
    "    if cache_key in tf_prob_cached.keys():\n",
    "        return tf_prob_cached[cache_key]\n",
    "\n",
    "    # Otherwise we'll calculate the probability\n",
    "    proba = 1\n",
    "    for i in range(1, len(text)):\n",
    "        base_str = text[:i]\n",
    "        next_char = text[i]\n",
    "        proba = proba * continuation_proba(base_str, next_char)\n",
    "\n",
    "    # Adjust for length\n",
    "    if length_adj:\n",
    "        proba = len(text) * proba\n",
    "\n",
    "    # Store in cache\n",
    "    tf_prob_cached[cache_key] = proba\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_breakdowns(segment):\n",
    "    # Remove apostrophes\n",
    "    options = []\n",
    "    max_len = min([len(segment), 4])\n",
    "    for i in range(1, max_len+1):\n",
    "        potential = segment[:i]\n",
    "        if potential in lat_eth.keys():\n",
    "            remainder = segment[i:]\n",
    "            if remainder == \"\":\n",
    "                options.append([potential])\n",
    "            else:\n",
    "                enders = calc_breakdowns(remainder)\n",
    "                if enders == []:\n",
    "                    return []\n",
    "                else:\n",
    "                    options.extend([potential, *e] for e in enders)\n",
    "    return options\n",
    "\n",
    "def get_breakdowns(segment):\n",
    "    results = calc_breakdowns(segment)\n",
    "    if len(results) == 0:\n",
    "        results = calc_breakdowns(segment.replace(\"'\", \"\").replace(\"`\", \"\"))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-greedy algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of 'current' states (e.g. top 3 'tibe' options)\n",
    "# And a list of things to add on (e.g. all 'bi' options)\n",
    "# What are the best n options for 'tibebi'\n",
    "def best_next_steps(current_states, next_options, n=3):\n",
    "    # Pairs keeps track of all texts + probabilities\n",
    "    pairs = []\n",
    "    \n",
    "    # From each possible \n",
    "    for base in current_states:\n",
    "        # Create all the text options we're looking at\n",
    "        # Then calculate their probability\n",
    "        texts = [f\"{base}{follower}\" for follower in next_options]\n",
    "        probs = [text_proba(text) for text in texts]\n",
    "        pairs.extend(zip(texts, probs))\n",
    "    \n",
    "    # Only return the top n options\n",
    "    top = sorted(pairs, key=lambda pair: pair[1], reverse=True)[:n]\n",
    "\n",
    "    return top\n",
    "\n",
    "# Receive a list like ['ti', 'be', 'bi', 'ni']\n",
    "# Tries out each Ethiopic option, returns top n most likely\n",
    "def get_top_sequences(breakdown, n=3, lower_bound=None):\n",
    "    eth_poss = [lat_eth[latin] for latin in breakdown]\n",
    "\n",
    "    # current = eth_poss[0]\n",
    "    # for next_options in eth_poss[1:]:\n",
    "    # Starts with a space bc beginning of word\n",
    "    current = \" \"\n",
    "    for next_options in eth_poss:\n",
    "        # If it's a space take all options\n",
    "        n_steps = 10 if current == \" \" else n\n",
    "        top = best_next_steps(current, next_options, n_steps)\n",
    "\n",
    "        # Only keep ones above lower score bound\n",
    "        # if you want to use them all, lower_bound should be None\n",
    "        if lower_bound != None:\n",
    "            current = [option[0] for option in top if option[1] >= lower_bound]\n",
    "            # For debugging\n",
    "            dropped = len(top) - len(current)\n",
    "        else:\n",
    "            current = [option[0] for option in top]\n",
    "    \n",
    "    # Remove spaces\n",
    "    top = [(option[0][1:], option[1]) for option in top]\n",
    "    return top\n",
    "\n",
    "# Takes a list of potential breakdowns\n",
    "# n is passed to get_top_sequences\n",
    "# limit is used here\n",
    "def get_top_sequences_all(breakdowns, limit=5, n=3):\n",
    "    tops = []\n",
    "    \n",
    "    # shorter breakdowns go first\n",
    "    breakdowns = sorted(breakdowns, key=lambda breakdown: len(breakdowns))\n",
    "\n",
    "    lower_bound = 0\n",
    "    # Visual readout of length\n",
    "    for breakdown in breakdowns:\n",
    "        top = get_top_sequences(breakdown, n, lower_bound=lower_bound)\n",
    "        if len(top) > 0:\n",
    "            worst_score = top[-1][1]\n",
    "            if worst_score > lower_bound:\n",
    "                lower_bound = worst_score\n",
    "            tops.extend(top)\n",
    "        \n",
    "    return sorted(tops, key=lambda pair: pair[1], reverse=True)[:limit]\n",
    "\n",
    "# Given Latin script, what is the best Ethiopic?\n",
    "def top_transcriptions(text, limit=5, n=3):\n",
    "    breakdowns = get_breakdowns(text)\n",
    "    return get_top_sequences_all(breakdowns, limit, n)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    sentence = [i for j in text.split() for i in (j, ' ')][:-1]\n",
    "    cleaned = []\n",
    "    for elmt in sentence:\n",
    "        elmt_tokenized = word_tokenize(elmt)\n",
    "        if elmt == ' ':\n",
    "            cleaned.append(' ')\n",
    "        elif len(elmt) == len(elmt_tokenized):\n",
    "            cleaned.append(elmt)\n",
    "        else:\n",
    "            for i in elmt_tokenized:\n",
    "                if i == \"'\":\n",
    "                    elmt_tokenized.remove(i)\n",
    "                cleaned += elmt_tokenized\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "def tensorflow_transliterate(text):\n",
    "    sent_trans = []\n",
    "\n",
    "    cleaned = tokenize_text(text)\n",
    "    for word in cleaned:\n",
    "        sent_trans.append(tensorflow_transliterate_word(word))\n",
    "    \n",
    "    return \"\".join(sent_trans)\n",
    "\n",
    "def tensorflow_transliterate_word(word, top_n=1):\n",
    "    if word.isspace():\n",
    "        return word\n",
    "    if word in lat_eth.keys() and len(lat_eth[word]) == 0:\n",
    "        return lat_eth[word][0]\n",
    "    elif word in string.punctuation:\n",
    "        return word\n",
    "    elif word.isnumeric() == True:\n",
    "        return word\n",
    "    else:\n",
    "        cleaned = word.lower()\n",
    "        results = top_transcriptions(cleaned)\n",
    "        if len(results) == 0:\n",
    "            cleaned = re.sub(r'[^a-z]', '', cleaned)\n",
    "            results = top_transcriptions(cleaned)\n",
    "\n",
    "        if len(results) == 0:\n",
    "            results = [word]\n",
    "\n",
    "        if top_n == 1:\n",
    "            if len(results) == 0:\n",
    "                return [word]\n",
    "            else:\n",
    "                return results[0][0]\n",
    "        else:\n",
    "            return [result[0] for result in results[:top_n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['t', 'i', 'b', 'e', 'b', 'i', 'n', 'i'],\n",
       " ['t', 'i', 'b', 'e', 'b', 'i', 'ni'],\n",
       " ['t', 'i', 'b', 'e', 'bi', 'n', 'i'],\n",
       " ['t', 'i', 'b', 'e', 'bi', 'ni'],\n",
       " ['t', 'i', 'be', 'b', 'i', 'n', 'i'],\n",
       " ['t', 'i', 'be', 'b', 'i', 'ni'],\n",
       " ['t', 'i', 'be', 'bi', 'n', 'i'],\n",
       " ['t', 'i', 'be', 'bi', 'ni'],\n",
       " ['ti', 'b', 'e', 'b', 'i', 'n', 'i'],\n",
       " ['ti', 'b', 'e', 'b', 'i', 'ni'],\n",
       " ['ti', 'b', 'e', 'bi', 'n', 'i'],\n",
       " ['ti', 'b', 'e', 'bi', 'ni'],\n",
       " ['ti', 'be', 'b', 'i', 'n', 'i'],\n",
       " ['ti', 'be', 'b', 'i', 'ni'],\n",
       " ['ti', 'be', 'bi', 'n', 'i'],\n",
       " ['ti', 'be', 'bi', 'ni']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting potential breakdowns\n",
    "breakdowns = get_breakdowns('tibebini')\n",
    "breakdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.103438150809997e-06"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate text probability\n",
    "text_proba('ጥበብ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ጥብብ', 4.8523289071696345e-06),\n",
       " ('ጥበብ', 4.103438150809997e-06),\n",
       " ('ጥብቢ', 8.614675079611741e-07)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given the starting options of ['ጥበ', 'ጥብ', 'ትበ']\n",
    "# What options out of ['ቢ', 'ብ'] are the best next step?\n",
    "best_next_steps(\n",
    "    ['ጥበ', 'ጥብ', 'ትበ'],\n",
    "    ['ቢ', 'ብ']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at ['t', 'i', 'b', 'e', 'b', 'i', 'n', 'i']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ጥእብኤብእንእ', 1.1083901216124514e-17),\n",
       " ('ትእብኤብእንእ', 3.663522329343407e-18),\n",
       " ('ጥዕብኤብእንእ', 1.0411927491793497e-18),\n",
       " ('ጥእብኤብእንዕ', 1.0411919735318827e-18),\n",
       " ('ጥእብኤብዕንእ', 1.0411919735318827e-18)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_top_sequences gets the top n options for a breakdown\n",
    "breakdown = breakdowns[0]\n",
    "print(\"Looking at\", breakdown)\n",
    "get_top_sequences(breakdown, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at 16 breakdowns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ጥበቢኒ', 2.9720881868637036e-09),\n",
       " ('ጥበብኒ', 2.1003423397835974e-09),\n",
       " ('ጥበቢን', 1.5000106938408966e-09)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_top_sequences_all does it for multiple breakdowns\n",
    "print(\"Looking at\", len(breakdowns), \"breakdowns\")\n",
    "get_top_sequences_all(breakdowns, limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ጥበቢ', 4.8237780542115944e-06),\n",
       " ('ጥበብ', 3.4089113942715496e-06),\n",
       " ('ትበቢ', 1.594386152385783e-06),\n",
       " ('ጥእበቢ', 3.0541725734773963e-07),\n",
       " ('ጥበብእ', 2.158353151351827e-07)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_transcriptions(\"tibebi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'እኒዲሕ ሲል . ሁለጥ ሰኦጭ ሊሰሊዩ ወደ'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorflow_transliterate(\"inidihi sil . hulat sewochi liseliyu wade\")\n",
    "#እንዲህ ሲል ። ሁለት ሰዎች ሊጸልዩ ወደ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenNMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model demo-model_step_25000.pt\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "model_name = subprocess.run(\"ls -t *model*.pt | head -n 1\",\n",
    "                            shell=True,\n",
    "                            stdout=subprocess.PIPE).stdout.decode(\"utf-8\").strip()\n",
    "print(\"Using model\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opennmt_transliterate(text):\n",
    "    # Split each word to separate line, spaces between chars\n",
    "    cleaned = unidecode.unidecode(text).lower()\n",
    "    t_words = [w for w in re.split(r\"\\s+\", cleaned) if w]\n",
    "    spaced = [' '.join(w) for w in t_words]\n",
    "    \n",
    "    with open(\"opennmt-test.txt\", 'w') as f:\n",
    "        f.write('\\n'.join(spaced))\n",
    "\n",
    "    !onmt_translate \\\n",
    "        -model {model_name} \\\n",
    "        -src opennmt-test.txt \\\n",
    "        -output opennmt-pred.txt \\\n",
    "        -replace_unk \\\n",
    "        > /dev/null 2>&1\n",
    "\n",
    "    with open(\"opennmt-pred.txt\") as f:\n",
    "        results = f.read()\n",
    "\n",
    "    # Remove spaces, return to being on one line\n",
    "    results = results.replace(' ', '').replace('\\n', ' ')\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'እንዲህ ሲል . ሁለት ሰዎች ሊጸልዩ ወደ '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opennmt_transliterate(\"inidihi sil . hulat sewochi liseliyu wade\")\n",
    "#እንዲህ ሲል ። ሁለት ሰዎች ሊጸልዩ ወደ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes two files (parallel texts) and generates\n",
    "# two lists after stripping beginning/trailing whitespace\n",
    "def extract_from_files(ethiopic_file, latin_file):\n",
    "  eth_file = open(ethiopic_file)\n",
    "  lat_file = open(latin_file)\n",
    "  ethiopic = [line.rstrip() for line in eth_file.read().splitlines()]\n",
    "  latin = [line.rstrip() for line in lat_file.read().splitlines()] \n",
    "  \n",
    "  ethiopic = list(ethiopic)\n",
    "  latin = list(latin)\n",
    "\n",
    "  return ethiopic, latin\n",
    "\n",
    "# this function takes two parallel lists and evaluates how\n",
    "# our model performs, given a transliteration function\n",
    "def evaluate(ethiopic, latin, translit_func):\n",
    "  accuracies = []\n",
    "  count = 0\n",
    "  total_length = 0\n",
    "  total_words = 0\n",
    "  total_accurate_words = 0\n",
    "\n",
    "  for line in ethiopic:\n",
    "    total_length += len(line)\n",
    "\n",
    "  paired = list(zip(ethiopic, latin))\n",
    "  for correct, latin in tqdm_notebook(paired):\n",
    "    predict = translit_func(latin)\n",
    "    weight = len(correct) / total_length\n",
    "    accuracy = Levenshtein.ratio(predict,correct) * weight\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    words_correct = re.split(r'\\s+', correct)\n",
    "    words_predicted = re.split(r'\\s+', predict)\n",
    "\n",
    "    for word_correct, word_predicted in zip(words_correct, words_predicted):\n",
    "        total_words += 1\n",
    "        if word_correct == word_predicted:\n",
    "            total_accurate_words += 1\n",
    "    \n",
    "  return {\n",
    "      'lev_acc': sum(accuracies),\n",
    "      'word_acc': total_accurate_words / total_words\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    {\n",
    "        \"name\": \"Simulated natural transliteration\",\n",
    "        \"paths\": [\n",
    "            \"raw/original.txt\",\n",
    "            \"raw/transliterated.txt\"\n",
    "        ],\n",
    "        \"postprocess\": (lambda rows: rows[:10])\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"The Lord's Prayer\",\n",
    "        \"paths\": [\n",
    "            \"parallel_data/lords_prayer_am.txt\",\n",
    "            \"parallel_data/lords_prayer_rom.txt\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": '\"Sera\"',\n",
    "        \"paths\": [\n",
    "            \"parallel_data/sera_am.txt\",\n",
    "            \"parallel_data/sera_rom.txt\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": '\"Taitu\"',\n",
    "        \"paths\": [\n",
    "            \"parallel_data/taitu_am.txt\",\n",
    "            \"parallel_data/taitu_rom.txt\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": '\"Tewodros\"',\n",
    "        \"paths\": [\n",
    "            \"parallel_data/tewodros_am.txt\",\n",
    "            \"parallel_data/tewodros_rom.txt\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": '\"Yasstesseriyal\"',\n",
    "        \"paths\": [\n",
    "            \"parallel_data/yasstesseriyal_am.txt\",\n",
    "            \"parallel_data/yasstesseriyal_rom.txt\"\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Simulated natural transliteration\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00a288329454e309b45cc6808d9fdc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1626714bdec476fa30d90f1bac9d910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baee2e095d8f4104b890aa834a5c9923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing The Lord's Prayer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b00604394844d49b4ca4fada48ef67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d208c5c8a54c4be4a0e7721b354b19c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b02b86a0d549bcba5f5c84468eae28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing \"Sera\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7323463a9d154dfc84bf6a3c7344aa2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=127), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b41d3001c164344b2252c430168d458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=127), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53a1d779c6b4af289fe2fb90f93ddfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=127), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing \"Taitu\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271d44a575a048acb9515417bb032c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=95), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381a1dde9ff04db8897d3b641dab8580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=95), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9832f016ba0443c4b7fc56429e8da127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=95), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing \"Tewodros\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e87c6ff859407f8e83b78db2fa9cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=56), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535c2307942c4127b23fa4c1ae0a0940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=56), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd13aade6ca84377b2f5010d7e55a505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=56), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing \"Yasstesseriyal\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7590340721cc4ddba88c7dff66edef1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447364612211433489da675fc9de0b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa61feba9cf448fe9a5b12e85925e39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lev_acc</th>\n",
       "      <th>word_acc</th>\n",
       "      <th>method</th>\n",
       "      <th>dataset</th>\n",
       "      <th>wordcount</th>\n",
       "      <th>charcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.739485</td>\n",
       "      <td>0.465517</td>\n",
       "      <td>Character N-Grams</td>\n",
       "      <td>Simulated natural transliteration</td>\n",
       "      <td>175</td>\n",
       "      <td>1572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.570635</td>\n",
       "      <td>0.080460</td>\n",
       "      <td>Tensorflow</td>\n",
       "      <td>Simulated natural transliteration</td>\n",
       "      <td>175</td>\n",
       "      <td>1572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.917378</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>OpenNMT</td>\n",
       "      <td>Simulated natural transliteration</td>\n",
       "      <td>175</td>\n",
       "      <td>1572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.685354</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>Character N-Grams</td>\n",
       "      <td>The Lord's Prayer</td>\n",
       "      <td>33</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.426812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Tensorflow</td>\n",
       "      <td>The Lord's Prayer</td>\n",
       "      <td>33</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.765185</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>OpenNMT</td>\n",
       "      <td>The Lord's Prayer</td>\n",
       "      <td>33</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.687849</td>\n",
       "      <td>0.327225</td>\n",
       "      <td>Character N-Grams</td>\n",
       "      <td>\"Sera\"</td>\n",
       "      <td>371</td>\n",
       "      <td>2630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.477971</td>\n",
       "      <td>0.078534</td>\n",
       "      <td>Tensorflow</td>\n",
       "      <td>\"Sera\"</td>\n",
       "      <td>371</td>\n",
       "      <td>2630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.742630</td>\n",
       "      <td>0.410995</td>\n",
       "      <td>OpenNMT</td>\n",
       "      <td>\"Sera\"</td>\n",
       "      <td>371</td>\n",
       "      <td>2630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.448463</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>Character N-Grams</td>\n",
       "      <td>\"Taitu\"</td>\n",
       "      <td>283</td>\n",
       "      <td>2284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.413573</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>Tensorflow</td>\n",
       "      <td>\"Taitu\"</td>\n",
       "      <td>283</td>\n",
       "      <td>2284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.674160</td>\n",
       "      <td>0.244300</td>\n",
       "      <td>OpenNMT</td>\n",
       "      <td>\"Taitu\"</td>\n",
       "      <td>283</td>\n",
       "      <td>2284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.695831</td>\n",
       "      <td>0.450382</td>\n",
       "      <td>Character N-Grams</td>\n",
       "      <td>\"Tewodros\"</td>\n",
       "      <td>262</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.506261</td>\n",
       "      <td>0.167939</td>\n",
       "      <td>Tensorflow</td>\n",
       "      <td>\"Tewodros\"</td>\n",
       "      <td>262</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.817897</td>\n",
       "      <td>0.576336</td>\n",
       "      <td>OpenNMT</td>\n",
       "      <td>\"Tewodros\"</td>\n",
       "      <td>262</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.539644</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>Character N-Grams</td>\n",
       "      <td>\"Yasstesseriyal\"</td>\n",
       "      <td>177</td>\n",
       "      <td>1392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.492073</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>Tensorflow</td>\n",
       "      <td>\"Yasstesseriyal\"</td>\n",
       "      <td>177</td>\n",
       "      <td>1392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.684546</td>\n",
       "      <td>0.180791</td>\n",
       "      <td>OpenNMT</td>\n",
       "      <td>\"Yasstesseriyal\"</td>\n",
       "      <td>177</td>\n",
       "      <td>1392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lev_acc  word_acc             method                            dataset  \\\n",
       "0   0.739485  0.465517  Character N-Grams  Simulated natural transliteration   \n",
       "1   0.570635  0.080460         Tensorflow  Simulated natural transliteration   \n",
       "2   0.917378  0.706897            OpenNMT  Simulated natural transliteration   \n",
       "3   0.685354  0.121212  Character N-Grams                  The Lord's Prayer   \n",
       "4   0.426812  0.000000         Tensorflow                  The Lord's Prayer   \n",
       "5   0.765185  0.142857            OpenNMT                  The Lord's Prayer   \n",
       "6   0.687849  0.327225  Character N-Grams                             \"Sera\"   \n",
       "7   0.477971  0.078534         Tensorflow                             \"Sera\"   \n",
       "8   0.742630  0.410995            OpenNMT                             \"Sera\"   \n",
       "9   0.448463  0.180000  Character N-Grams                            \"Taitu\"   \n",
       "10  0.413573  0.093333         Tensorflow                            \"Taitu\"   \n",
       "11  0.674160  0.244300            OpenNMT                            \"Taitu\"   \n",
       "12  0.695831  0.450382  Character N-Grams                         \"Tewodros\"   \n",
       "13  0.506261  0.167939         Tensorflow                         \"Tewodros\"   \n",
       "14  0.817897  0.576336            OpenNMT                         \"Tewodros\"   \n",
       "15  0.539644  0.187500  Character N-Grams                   \"Yasstesseriyal\"   \n",
       "16  0.492073  0.039773         Tensorflow                   \"Yasstesseriyal\"   \n",
       "17  0.684546  0.180791            OpenNMT                   \"Yasstesseriyal\"   \n",
       "\n",
       "    wordcount  charcount  \n",
       "0         175       1572  \n",
       "1         175       1572  \n",
       "2         175       1572  \n",
       "3          33        290  \n",
       "4          33        290  \n",
       "5          33        290  \n",
       "6         371       2630  \n",
       "7         371       2630  \n",
       "8         371       2630  \n",
       "9         283       2284  \n",
       "10        283       2284  \n",
       "11        283       2284  \n",
       "12        262       2022  \n",
       "13        262       2022  \n",
       "14        262       2022  \n",
       "15        177       1392  \n",
       "16        177       1392  \n",
       "17        177       1392  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "methods = [\n",
    "    ['Character N-Grams', char_ngram_transliterate],\n",
    "    ['Tensorflow', tensorflow_transliterate],\n",
    "#    ['Word N-Grams', word_ngram_transliterate],\n",
    "   ['OpenNMT', opennmt_transliterate]\n",
    "]\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Processing\", dataset['name'])\n",
    "    ethiopic, latin = extract_from_files(dataset['paths'][0], dataset['paths'][1])\n",
    "    if 'postprocess' in dataset:\n",
    "        ethiopic = dataset['postprocess'](ethiopic)\n",
    "        latin = dataset['postprocess'](latin)\n",
    "\n",
    "    for method in methods:\n",
    "        result = evaluate(ethiopic, latin, translit_func=method[1])\n",
    "        result['method'] = method[0]\n",
    "        result['dataset'] = dataset['name']\n",
    "        result['wordcount'] = len(re.split('\\s+', ' '.join(latin)))\n",
    "        result['charcount'] = len('\\n'.join(latin))\n",
    "        results.append(result)\n",
    "\n",
    "scores = pd.DataFrame(results)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For manual comparison of texts saved as files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01828966880869995"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_full_texts(preds, origs):\n",
    "    total = 0\n",
    "    matched = 0\n",
    "    for pred, orig in zip(preds, origs):\n",
    "        pred_words = [w for w in re.split('\\s+', pred) if w]\n",
    "        orig_words = [w for w in re.split('\\s+', orig) if w]\n",
    "        for word in pred_words:\n",
    "            if word in orig_words:\n",
    "                orig_words.remove(word)\n",
    "                matched += 1\n",
    "        total += matched + len(orig_words)\n",
    "\n",
    "    return matched / total\n",
    "        \n",
    "pred = open(\"temp/taitu_pred.txt\").read().replace(\"፡\", \"\").strip().splitlines()\n",
    "orig = open(\"parallel_data/taitu_am.txt\").read().replace(\"፡\", \"\").strip().splitlines()\n",
    "compare_full_texts(pred, orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "02-OpenNMT training by full sentences.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
